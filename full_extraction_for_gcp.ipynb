{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5b0ea73",
   "metadata": {},
   "source": [
    "# Notebook pour réaliser l'extraction des posts et commentaires\n",
    "### Adapté pour être facilement utilisable sur Jupyter dans GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58f3904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4747c32d",
   "metadata": {},
   "source": [
    "## Importer un fichier depuis le bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb317ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The ID of your GCS object\n",
    "    # source_blob_name = \"storage-object-name\"\n",
    "\n",
    "    # The path to which the file should be downloaded\n",
    "    # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Construct a client side representation of a blob.\n",
    "    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "    # any content from Google Cloud Storage. As we don't need additional data,\n",
    "    # using `Bucket.blob` is preferred here.\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ff523",
   "metadata": {},
   "source": [
    "## Fonction d'authentification  à l'API reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7228b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_API():\n",
    "    \"\"\"\n",
    "    Function to authenticate to the Reddit API\n",
    "    Returns the headers we'll need for future API queries.\n",
    "    \"\"\"\n",
    "    # Open the file containing the login infos, and store them for later on\n",
    "    download_blob('the-clean-project','notebooks/jupyter/authentication_file.txt','auth_file.txt')\n",
    "    \n",
    "    with open('auth_file.txt', 'r') as f: \n",
    "        client_id, secret_token, grant_type, username, password, user_agent = f.readline().split(\",\")\n",
    "        \n",
    "    # note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "    auth = requests.auth.HTTPBasicAuth(client_id, secret_token)\n",
    "\n",
    "    # here we pass our login method (password), username, and password\n",
    "    data = {'grant_type': grant_type,\n",
    "            'username': username,\n",
    "            'password': password}\n",
    "\n",
    "    # setup our header info, which gives reddit a brief description of our app\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    # send our request for an OAuth token\n",
    "    res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                        auth=auth, data=data, headers=headers)\n",
    "\n",
    "    # convert response to JSON and pull access_token value\n",
    "    TOKEN = res.json()['access_token']\n",
    "\n",
    "    # add authorization to our headers dictionary\n",
    "    headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "    return headers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52773f75",
   "metadata": {},
   "source": [
    "## Le code d'extraction des posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ea5d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_response_for_posts(res):\n",
    "    \"\"\"\n",
    "    We use this function to convert responses to dataframes, in the context of POSTS extraction.\n",
    "    In this dataframe, we extract metrics from the json file given as a response\n",
    "    \n",
    "    Arg : res, which is the query we make to the Reddit API (a GET query).\n",
    "    Returns : a dataframe containing the info for each extracted post, that is (for now):\n",
    "    - subreddit: subreddit name;\n",
    "    - title: post title;\n",
    "    - selftext: post body;\n",
    "    - author_fullname: id of the post author (t2_'xxxxx');\n",
    "    - upvote_ratio; \n",
    "    - created_utc: publication time;\n",
    "    - num_comments;\n",
    "    - id: post ID, which is part of each post URL btw;\n",
    "    - kind: type prefix, for posts of a subreddit it is \"t3\", \"t1\" for a comment...\n",
    "    \"\"\"\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # loop through each post pulled from res and append to df\n",
    "    for post in res.json()['data']['children']:\n",
    "        df_new = pd.DataFrame({\n",
    "            'subreddit': post['data']['subreddit'],\n",
    "            # Take the subreddit ID?\n",
    "            'title': post['data']['title'],\n",
    "#             'selftext': post['data']['selftext'],\n",
    "            'author': post['data']['author'],\n",
    "            'upvote_ratio': post['data']['upvote_ratio'],\n",
    "            'created_utc': datetime.fromtimestamp(post['data']['created_utc']).strftime('%Y-%m-%dT%H:%M:%SZ'),\n",
    "            'num_comments': post['data']['num_comments'],\n",
    "            'id': post['data']['id'],\n",
    "            'kind': post['kind']\n",
    "        }, index=[1])\n",
    "        df = pd.concat([df, df_new], ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def posts_extraction(subreddit:str=\"r/worldnews\"):\n",
    "    \"\"\"\n",
    "    Function to extract the first posts from a subreddit page.\n",
    "    \n",
    "    Args :\n",
    "    - subreddit: name of the subreddit under format \"r/name\", by default \"r/worldnews\";\n",
    "    - n_posts: int, number of posts we want to extract from the subreddit, by default 100.\n",
    "        A choice we make here is to automatically round this number to the upper hundred in our code.\n",
    "    Creates : a csv file containing the info about the extracted posts, that is:\n",
    "    -\n",
    "    -\n",
    "    -\n",
    "\n",
    "    TODO: try-except, error prevention \n",
    "    \"\"\"\n",
    "\n",
    "    # We first authenticate to the API\n",
    "    headers = authenticate_API()\n",
    "\n",
    "    # initialize dataframe and parameters for pulling data in loop \n",
    "    data = pd.DataFrame()\n",
    "    params = {'limit': 10}\n",
    "\n",
    "    # Create a flag for scanning the subreddit as long as there is a post to fetch\n",
    "    flag = True\n",
    "\n",
    "    # At each loop, we extract 100 posts with their info\n",
    "#     while flag:\n",
    "#         # make request\n",
    "#         res = requests.get(f\"https://oauth.reddit.com/{subreddit}\",\n",
    "#                         headers=headers,\n",
    "#                         params=params)\n",
    "\n",
    "#         # get dataframe from response\n",
    "#         new_df = df_from_response_for_posts(res)\n",
    "#         # take the final row (oldest entry)\n",
    "#         row = new_df.iloc[len(new_df)-1]\n",
    "#         # create fullname\n",
    "#         fullname = row['kind'] + '_' + row['id']\n",
    "#         # add/update fullname in params\n",
    "#         params['after'] = fullname\n",
    "        \n",
    "#         # append new_df to data\n",
    "#         # data = data.append(new_df, ignore_index=True)\n",
    "#         data = pd.concat([data, new_df], ignore_index=True)\n",
    "\n",
    "#         # Flag set to True if len(new_df)>=100, False otherwise\n",
    "#         flag = (len(new_df)>=100)\n",
    "        \n",
    "    # make request\n",
    "    res = requests.get(f\"https://oauth.reddit.com/{subreddit}\",\n",
    "                    headers=headers,\n",
    "                    params=params)\n",
    "\n",
    "    # get dataframe from response\n",
    "    new_df = df_from_response_for_posts(res)\n",
    "    # take the final row (oldest entry)\n",
    "    row = new_df.iloc[len(new_df)-1]\n",
    "    # create fullname\n",
    "    fullname = row['kind'] + '_' + row['id']\n",
    "    # add/update fullname in params\n",
    "    params['after'] = fullname\n",
    "\n",
    "    # append new_df to data\n",
    "    # data = data.append(new_df, ignore_index=True)\n",
    "    data = pd.concat([data, new_df], ignore_index=True)\n",
    "\n",
    "    # Flag set to True if len(new_df)>=100, False otherwise\n",
    "    flag = (len(new_df)>=100)\n",
    "\n",
    "    # Save final dataframe to csv file, \n",
    "    # name_format: data_API_whole_\"subreddit name\"_\"date and hour\".csv\n",
    "    path = \"gs://the-clean-project/posts_files/\"\n",
    "    filename = os.path.join(path, f'dataAPI_whole_{subreddit.replace(\"/\",\"_\")}_'+ datetime.today().strftime('%Y-%m-%d') + \".csv\")\n",
    "\n",
    "    data.to_csv(filename, sep = ',')\n",
    "    \n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c12dde8",
   "metadata": {},
   "source": [
    "## Le code d'extraction des commentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bfd5f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_replies(resjson, df_comments, id_post, rang, headers):\n",
    "    \"\"\"\n",
    "    Permet d'extraire les réponses aux commentaires contenus dans le json resjson\n",
    "\n",
    "    Requête : comments\n",
    "\n",
    "    Params :\n",
    "        - resjson : json\n",
    "        - df_comments : dataframe\n",
    "        - id_post : str\n",
    "        - rang : int\n",
    "    \n",
    "    Return : le dataframe contenant les informations des réponses aux commentaires\n",
    "    \"\"\"\n",
    "    # add content of the comment in the dataframe\n",
    "    if \"body\" in resjson['data']:\n",
    "        df_add = pd.DataFrame({\n",
    "            'id': id_post,\n",
    "            'contenu' : resjson['data']['body'],\n",
    "            'type': \"Comment\",\n",
    "            'rang': rang\n",
    "        }, index=[1])\n",
    "\n",
    "        df_comments = pd.concat([df_comments, df_add], ignore_index=True)\n",
    "    else:\n",
    "        df_comments = children_extraction(df_comments, resjson['data']['id'],resjson['data']['children'], rang + 1, headers, id_post)\n",
    "\n",
    "    if \"replies\" in resjson['data']:\n",
    "        # recursive to add replies\n",
    "        if resjson['data']['replies'] != \"\":\n",
    "            for post in resjson['data']['replies']['data']['children']:\n",
    "                df_comments = df_replies(post, df_comments, id_post, rang + 1, headers)\n",
    "    else:\n",
    "        df_comments = children_extraction(df_comments, resjson['data']['id'],resjson['data']['children'], rang + 1, headers, id_post)\n",
    "\n",
    "    return df_comments\n",
    "\n",
    "def df_from_response_for_comments(res, headers):\n",
    "    \"\"\"\n",
    "    Permet d'extraire les commentaires contenus dans le json res\n",
    "\n",
    "    Requête : comments\n",
    "\n",
    "    Params :\n",
    "        - res : json\n",
    "\n",
    "    Return : le dataframe contenant les informations des commentaires\n",
    "    \"\"\"\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df_comments = pd.DataFrame()\n",
    "    \n",
    "    # add_type title of the post\n",
    "    df_add = pd.DataFrame({\n",
    "        'id': res.json()[0]['data']['children'][0]['data']['id'], \n",
    "        'contenu' : res.json()[0]['data']['children'][0]['data']['title'],\n",
    "        'type': \"Post\",\n",
    "        'rang': 0\n",
    "    }, index = [1])\n",
    "\n",
    "    df_comments = pd.concat([df_comments, df_add], ignore_index=True)\n",
    "\n",
    "    # add_type content of the post\n",
    "    df_add = pd.DataFrame({\n",
    "        'id': res.json()[0]['data']['children'][0]['data']['id'], \n",
    "        'contenu' : res.json()[0]['data']['children'][0]['data']['selftext'],\n",
    "        'type': \"Post\",\n",
    "        'rang': 0\n",
    "    }, index = [1])\n",
    "\n",
    "    df_comments = pd.concat([df_comments, df_add], ignore_index=True)\n",
    "\n",
    "    id_post = res.json()[0]['data']['children'][0]['data']['id']\n",
    "\n",
    "    # loop to add content of the comments\n",
    "    for post in res.json()[1]['data']['children']:\n",
    "        df_comments = df_replies(post, df_comments, id_post, 1, headers)\n",
    "    return df_comments\n",
    "\n",
    "def df_from_response_for_children(res, id_post, rang, headers):\n",
    "    \"\"\"\n",
    "    Permet d'extraire les commentaires contenus dans le json res\n",
    "\n",
    "    Requête : morechildren\n",
    "\n",
    "    Params :\n",
    "        - res : json\n",
    "        - id_post : str\n",
    "        - rang : int\n",
    "\n",
    "    Return : le dataframe contenant les informations des commentaires\n",
    "    \"\"\"\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df_children = pd.DataFrame()\n",
    "    \n",
    "    if 'json' in res.json():\n",
    "        if len(res.json()['json']['data']['things']) != 0:\n",
    "            for post in res.json()['json']['data']['things']:\n",
    "                if 'body' in post['data']:\n",
    "                    df_add = pd.DataFrame({\n",
    "                        'id': id_post,\n",
    "                        'contenu': post['data']['body'],\n",
    "                        'type': \"Comment\",\n",
    "                        'rang': rang + 1\n",
    "                    }, index=[1])\n",
    "                    df_children = pd.concat([df_children, df_add], ignore_index=True)\n",
    "                else:\n",
    "                    df_children = children_extraction(df_children, post['data']['id'], post['data']['children'], rang + 1, headers, id_post)\n",
    "\n",
    "    return df_children\n",
    "\n",
    "def children_extraction(df_comments, link_id, liste_children, rang, headers, id_post):\n",
    "    \"\"\"\n",
    "    Permet d'extraire les commentaires non accessibles avec la première requête\n",
    "    Utilisation de l'API avec la requêtre 'more children'\n",
    "\n",
    "    Params :\n",
    "        - df_comments : dataframe\n",
    "        - link_id : str\n",
    "        - children : list\n",
    "        - rang : int\n",
    "\n",
    "    Return : le dataframe avec les nouveaux commentaires\n",
    "    \"\"\"\n",
    "    children = \"\"\n",
    "    i = 0\n",
    "\n",
    "    while len(liste_children) > 0:\n",
    "        if len(liste_children) > 100:\n",
    "            children = liste_children[0]\n",
    "            for i in range(1, 100):\n",
    "                children += \", \"\n",
    "                children += liste_children[i]\n",
    "            del(liste_children[0:100])\n",
    "        else:\n",
    "            children = liste_children[0]\n",
    "            for i in range(1, len(liste_children)):\n",
    "                children += \", \"\n",
    "                children += liste_children[i]\n",
    "            liste_children.clear()\n",
    "\n",
    "        params = {'link_id': \"t3_\" + id_post, 'children': children, 'api_type': \"json\"}\n",
    "\n",
    "        # make request\n",
    "        res = requests.get(\"http://oauth.reddit.com/api/morechildren\",\n",
    "                            headers=headers, params=params)\n",
    "\n",
    "        # get dataframe from response\n",
    "        new_df = df_from_response_for_children(res, id_post, rang, headers)\n",
    "\n",
    "        # append new_df to data\n",
    "        df_comments = pd.concat([df_comments, new_df], ignore_index=True)\n",
    "\n",
    "    return df_comments\n",
    "\n",
    "def comments_extraction(subreddit:str=\"r/worldnews\", id_post:str=\"w7bl8z\"):\n",
    "    \"\"\"\n",
    "    Permet d'extraire tous les commentaires d'un post et de les stocker dans un fichier csv\n",
    "\n",
    "    Params :\n",
    "        - subreddit : str\n",
    "        - id_post : str\n",
    "    \"\"\"\n",
    "    # We first authenticate to the API\n",
    "    headers = authenticate_API()\n",
    "\n",
    "    # initialize dataframe and parameters for pulling data in loop\n",
    "    data = pd.DataFrame()\n",
    "    params = {'limit': None}\n",
    "\n",
    "    # make request\n",
    "    res = requests.get(f\"https://oauth.reddit.com/{subreddit}/comments/{id_post}\",\n",
    "                    headers=headers,\n",
    "                    params=params)\n",
    "\n",
    "    # get dataframe from response\n",
    "    new_df = df_from_response_for_comments(res, headers)\n",
    "\n",
    "    # append new_df to data\n",
    "    # data = data.append(new_df, ignore_index=True)\n",
    "    data = pd.concat([data, new_df], ignore_index=True)\n",
    "\n",
    "    # Save final dataframe to csv file\n",
    "    path = \"gs://the-clean-project/comments_files/testmultiFichiers/\"\n",
    "    filename = os.path.join(path, f'dataAPI_comments_{subreddit.replace(\"/\",\"_\")}_{id_post}_'+ datetime.today().strftime('%Y-%m-%d') + \".csv\")\n",
    "\n",
    "    data.to_csv(filename, sep = ',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b91717",
   "metadata": {},
   "source": [
    "## L'extraction complète"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b244710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "\n",
    "def extraction_complete(subreddit:str=\"r/worldnews\"):\n",
    "    \"\"\"\n",
    "        Fonction qui, quand on lui fournit le subreddit, va récupérer de façon automatique :\n",
    "        - ses posts;\n",
    "        - les commentaires associés à chacun des posts;\n",
    "        (- la liste de ses contributeurs, ainsi que\n",
    "        - leur activité récente respective) (abandonné : demande trop de requêtes API pour l'instant...)\n",
    "\n",
    "        param:\n",
    "            subreddit: str, nom du subreddit au format \"r/nom_subreddit\"\n",
    "    \n",
    "        Output:\n",
    "            Enregistre les données ainsi extraites dans des fichiers csv.\n",
    "\n",
    "        \n",
    "        #TODO :\n",
    "            - on récupère aussi les json ?\n",
    "            - prévoir d'organiser le système de fichiers où les sauvegarder (bucket dans GCP?)\n",
    "    \"\"\"\n",
    "    # ti = time.time()\n",
    "    # Extraire les posts du subreddit, enregistré dans un csv\n",
    "    posts_filename = posts_extraction(subreddit)\n",
    "\n",
    "#     # On récupère le fichier csv ainsi créé\n",
    "#     with open(posts_filename, encoding='utf-8') as f:\n",
    "        \n",
    "#         # Parcourir les posts du subreddit (en omettant la ligne d'entête)\n",
    "#         posts = csv.reader(f, delimiter=',')\n",
    "#         next(posts)\n",
    "#         # pour chaque post, en extraire les commentaires (sauvegarde dans un csv)\n",
    "#         for post in posts:\n",
    "#             # l'ID du post est en avant dernière position dans la liste des infos du post\n",
    "#             comments_extraction(subreddit, id_post=post[-2])\n",
    "#             # Mettre la sécurité d'une seconde entre les extractions, du fait de la limitation de l'API reddit\n",
    "#             # time.sleep(0.9)\n",
    "#     # print(time.time()-ti)\n",
    "\n",
    "    df = pd.read_csv(posts_filename, storage_options={\"token\": \"cloud\"})\n",
    "    \n",
    "    for i in df.index:\n",
    "#         print(i)\n",
    "        comments_extraction(subreddit, df[\"id\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d688ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction_complete(\"r/fcbasel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec57d348-c396-4618-b632-75e3d59074cd",
   "metadata": {},
   "source": [
    "## Import des données des 30 plus gros subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6be38cf2-6db5-4acf-99c3-e855cd4e6e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "top30Subreddits = [\"r/announcements\", \"r/funny\", \"r/AskReddit\", \"r/gaming\", \"r/aww\",\n",
    "                \"r/Music\", \"r/pics\", \"r/science\", \"r/worldnews\", \"r/videos\",\n",
    "                \"r/todayilearned\", \"r/movies\", \"r/news\", \"r/Showerthoughts\", \"r/EarthPorn\",\n",
    "                \"r/gifs\", \"r/IAmA\", \"r/food\", \"r/askscience\", \"r/Jokes\",\n",
    "                \"r/LifeProTips\", \"r/explainlikeimfive\", \"r/Art\", \"r/books\", \"r/mildlyinteresting\",\n",
    "                \"r/nottheonion\", \"r/DIY\", \"r/sports\", \"r/blog\", \"r/space\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3d2d67c-92f2-4049-af1e-98da9b5ba9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-09-09 08:06:48.986827'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d8c1b8-8a7e-4ab4-ab3d-4aa228d0b2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_complete(\"r/worldnews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3dc6b3-dcc2-43ac-bb80-165b4949faa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-09-09 08:29:44.073743'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}