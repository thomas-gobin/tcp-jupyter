{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fb0ed18-e191-43d3-bf04-6dca4926fbda",
   "metadata": {},
   "source": [
    "# Notebook pour réaliser le scoring des commentaires\n",
    "\n",
    "* __Description__: Notebook pour le scoring des commentaires avec l'algorithme jigsaw\n",
    "* __Source__: Table comment (BigQuery) \n",
    "* __Output__: Table comment (BigQuery) \n",
    "* __Auteur__: Thomas GOBIN et Camille MATTHIEU\n",
    "* __Date de création__: 15/09/2022\n",
    "* __Date de mise à jour__: 15/09/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614a4129-ff21-4f3f-ba60-d192e3cae2f4",
   "metadata": {},
   "source": [
    "## Import des outils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b7bde21-0229-4c3f-a814-844f10d6a3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run /notebook/Libs/scoring_lib.ipynb\n",
    "%run /notebook/Libs/requete_lib.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69329ab6-f71f-4ffd-aacb-54f634f79118",
   "metadata": {},
   "source": [
    "## Déclaration des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d86725f-346d-4604-9d8f-6d20da049b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_model_global = \"/model_ml/model_global\"\n",
    "path_model_toxic = \"/model_ml/model_toxic\"\n",
    "path_model_severe_toxic = \"/model_ml/model_severe_toxic\"\n",
    "path_model_obscene = \"/model_ml/model_obscene\"\n",
    "path_model_threat = \"/model_ml/model_threat\"\n",
    "path_model_insult = \"/model_ml/model_insult\"\n",
    "path_model_identity_hate = \"/model_ml/model_identity_hate\"\n",
    "path_vec_global = \"/model_ml/vec_global\"\n",
    "path_vec_toxic = \"/model_ml/vec_toxic\"\n",
    "path_vec_severe_toxic = \"/model_ml/vec_severe_toxic\"\n",
    "path_vec_obscene = \"/model_ml/vec_obscene\"\n",
    "path_vec_threat = \"/model_ml/vec_threat\"\n",
    "path_vec_insult = \"/model_ml/vec_insult\"\n",
    "path_vec_identity_hate = \"/model_ml/vec_identity_hate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b13881-6b2e-41e4-b956-fbe544ba8891",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "\n",
    "Cette fonction permet de calculer le score des contenus de reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d5c23da-5cfa-4edc-aa9c-fd556447318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring():\n",
    "    \"\"\"\n",
    "    Fonction qui va calculer le score global des contenus de reddit\n",
    "    \"\"\"\n",
    "    \n",
    "    # Récupération des données à analyser\n",
    "    sql = f\"SELECT id_comment, id_post, content, type_content  FROM `.dwh.comment` WHERE extraction_utc = '{date_extraction}';\"\n",
    "    df = client.query(sql).to_dataframe()\n",
    "\n",
    "    # Récupération des vecteurs\n",
    "    vec_global = load_model(path_vec_global)\n",
    "    vec_toxic = load_model(path_vec_toxic)\n",
    "    vec_severe_toxic = load_model(path_vec_severe_toxic)\n",
    "    vec_obscene = load_model(path_vec_obscene)\n",
    "    vec_threat = load_model(path_vec_threat)\n",
    "    vec_insult = load_model(path_vec_insult)\n",
    "    vec_identity_hate = load_model(path_vec_identity_hate)\n",
    "    \n",
    "    table = \"dwh.score_jigsaw\"\n",
    "    \n",
    "    # Modèle global\n",
    "    if model_exist(path_model_global):\n",
    "        print(\"Model global\")\n",
    "        # Récupération du modèle\n",
    "        model = load_model(path_model_global) \n",
    "        # Application du modèle\n",
    "        df_global = apply_model(df, model, vec_global, table)\n",
    "    else:\n",
    "        # Entrainement du modèle\n",
    "        create_model()\n",
    "        # Récupération du modèle\n",
    "        model = load_model(path_model_global) \n",
    "        # Application du modèle\n",
    "        df_global = apply_model(df, model, vec_global, table)\n",
    "    \n",
    "    # Modèle toxic\n",
    "    if model_exist(path_model_toxic):\n",
    "        print(\"Model toxic\")\n",
    "        # Récupération du modèle\n",
    "        model = load_model(path_model_toxic) \n",
    "        # Application du modèle\n",
    "        df_toxic = apply_model(df, model, vec_toxic, table)\n",
    "    else:\n",
    "        # Entrainement du modèle\n",
    "        create_model()\n",
    "        # Récupération du modèle\n",
    "        model = load_model(path_model_toxic) \n",
    "        # Application du modèle\n",
    "        df_toxic = apply_model(df, model, vec_toxic, table)\n",
    "    \n",
    "    # Modèle severe toxic\n",
    "    if model_exist(path_model_severe_toxic):\n",
    "        print(\"Model severe toxic\")\n",
    "        # Récupération du modèle\n",
    "        model = load_model(path_model_severe_toxic) \n",
    "        # Application du modèle\n",
    "        df_severe_toxic = apply_model(df, model, vec_severe_toxic, table)\n",
    "    else:\n",
    "        # Entrainement du modèle\n",
    "        create_model()\n",
    "        # Récupération du modèle\n",
    "        model = load_model(path_model_severe_toxic) \n",
    "        # Application du modèle\n",
    "        df_severe_toxic = apply_model(df, model, vec_severe_toxic, table)\n",
    "    \n",
    "    # Modèle obscene\n",
    "    if model_exist(path_model_obscene):\n",
    "        print(\"Model obscene\")\n",
    "        # Récupération du modèle\n",
    "        model = load_model(path_model_obscene) \n",
    "        # Application du modèle\n",
    "        df_obscene = apply_model(df, model, vec_obscene, table)\n",
    "    else:\n",
    "        # Entrainement du modèle\n",
    "        create_model()\n",
    "        # Récupération du modèle\n",
    "        model = load_model(path_model_obscene) \n",
    "        # Application du modèle\n",
    "        df_obscene = apply_model(df, model, vec_obscene, table)\n",
    "    \n",
    "    # Modèle threat\n",
    "    if model_exist(path_model_threat):\n",
    "        print(\"Model threat\")\n",
    "        # Récupération du modèle\n",
    "        model = load_model(path_model_threat) \n",
    "        # Application du modèle\n",
    "        df_threat = apply_model(df, model, vec_threat, table)\n",
    "    else:\n",
    "        # Entrainement du modèle\n",
    "        create_model()\n",
    "        # Récupération du modèle\n",
    "        model = load_model(path_model_threat) \n",
    "        # Application du modèle\n",
    "        df_threat = apply_model(df, model, vec_threat, table)\n",
    "    \n",
    "    # Modèle insult\n",
    "    if model_exist(path_model_insult):\n",
    "        print(\"Model insult\")\n",
    "        # Récupération du modèle\n",
    "        model = load_model(path_model_insult) \n",
    "        # Application du modèle\n",
    "        df_insult = apply_model(df, model, vec_insult, table)\n",
    "    else:\n",
    "        # Entrainement du modèle\n",
    "        create_model()\n",
    "        # Récupération du modèle\n",
    "        model = load_model(path_model_insult) \n",
    "        # Application du modèle\n",
    "        df_insult = apply_model(df, model, vec_insult, table)\n",
    "    \n",
    "    # Modèle identity hate\n",
    "    if model_exist(path_model_identity_hate):\n",
    "        print(\"Model identity hate\")\n",
    "        # Récupération du modèle\n",
    "        model = load_model(path_model_identity_hate) \n",
    "        # Application du modèle\n",
    "        df_identity_hate = apply_model(df, model, vec_identity_hate, table)\n",
    "    else:\n",
    "        # Entrainement du modèle\n",
    "        create_model()\n",
    "        # Récupération du modèle\n",
    "        model = load_model(path_model_identity_hate) \n",
    "        # Application du modèle\n",
    "        df_identity_hate = apply_model(df, model, vec_identity_hate, table)\n",
    "    \n",
    "    # Suppression et renommage des colonnes\n",
    "    df_global.rename(columns = {'score_jigsaw':'score_global'}, inplace = True)\n",
    "    del df_toxic['id_comment']\n",
    "    del df_toxic['id_post']\n",
    "    del df_toxic['type_content']\n",
    "    df_toxic.rename(columns = {'score_jigsaw':'score_toxic'}, inplace = True)\n",
    "    del df_severe_toxic['id_comment']\n",
    "    del df_severe_toxic['id_post']\n",
    "    del df_severe_toxic['type_content']\n",
    "    df_severe_toxic.rename(columns = {'score_jigsaw':'score_severe_toxic'}, inplace = True)\n",
    "    del df_obscene['id_comment']\n",
    "    del df_obscene['id_post']\n",
    "    del df_obscene['type_content']\n",
    "    df_obscene.rename(columns = {'score_jigsaw':'score_obscene'}, inplace = True)\n",
    "    del df_threat['id_comment']\n",
    "    del df_threat['id_post']\n",
    "    del df_threat['type_content']\n",
    "    df_threat.rename(columns = {'score_jigsaw':'score_threat'}, inplace = True)\n",
    "    del df_insult['id_comment']\n",
    "    del df_insult['id_post']\n",
    "    del df_insult['type_content']\n",
    "    df_insult.rename(columns = {'score_jigsaw':'score_insult'}, inplace = True)\n",
    "    del df_identity_hate['id_comment']\n",
    "    del df_identity_hate['id_post']\n",
    "    del df_identity_hate['type_content']\n",
    "    df_identity_hate.rename(columns = {'score_jigsaw':'score_identity_hate'}, inplace = True)\n",
    "    \n",
    "    # Fusion des résultats\n",
    "    df_global = df_global.merge(df_toxic, left_index=True, right_index=True)\n",
    "    df_global = df_global.merge(df_severe_toxic, left_index=True, right_index=True)\n",
    "    df_global = df_global.merge(df_obscene, left_index=True, right_index=True)\n",
    "    df_global = df_global.merge(df_threat, left_index=True, right_index=True)\n",
    "    df_global = df_global.merge(df_insult, left_index=True, right_index=True)\n",
    "    df_global = df_global.merge(df_identity_hate, left_index=True, right_index=True)\n",
    "    \n",
    "    print(df_global.columns.values)\n",
    "    \n",
    "    # Insertion des données dans BigQuery\n",
    "    insertion_bigquery(\"dwh\", \"score_jigsaw\", df_global)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
