{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3890aa30-f783-4645-a45c-3dde80631183",
   "metadata": {},
   "source": [
    "# Librairie pour réaliser l'extraction des posts et commentaires\n",
    "\n",
    "* __Description__: Librairie des fonctions utilisées pour l'extraction des commentaires et des posts de Reddit pour leur sauvegarde dans BigQuery\n",
    "* __Source__: API de REDDIT\n",
    "* __Output__: Tables comment et post (BigQuery) \n",
    "* __Auteur__: Corentin TIMAL et Camille MATTHIEU\n",
    "* __Date de création__: 03/08/2022\n",
    "* __Date de mise à jour__: 14/09/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb020aca-77f4-42b8-bca0-de7846f7e479",
   "metadata": {},
   "source": [
    "### Import des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c1dbe7c-0e3e-4926-94f2-7aca2b8e33d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "from google.cloud import storage\n",
    "import numpy as np\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "from google.cloud import bigquery\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f9452-e82c-44d0-bcb6-1cce1b77b148",
   "metadata": {},
   "source": [
    "### Création du client pour la connexion à bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b51242-b107-4c52-8e14-ac0f2f3317fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483b386b-49bb-4cc5-8016-ff5d46de6c06",
   "metadata": {},
   "source": [
    "### Import d'un fichier depuis le bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f0eaf8-f625-4534-ad4f-e170cc351a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"\n",
    "    Downloads a blob from the bucket\n",
    "    \"\"\"\n",
    "    \n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ee000d-8d24-4b6a-b5d7-eba7d82a320e",
   "metadata": {},
   "source": [
    "### Authentification pour l'API Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3f526c-6f20-4f41-9479-f2296dd4a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_API():\n",
    "    \"\"\"\n",
    "    Function to authenticate to the Reddit API\n",
    "    Returns the headers we'll need for future API queries\n",
    "    \"\"\"\n",
    "    # Open the file containing the login infos, and store them for later on\n",
    "    download_blob('the-clean-project','notebooks/jupyter/authentication_file.txt','auth_file.txt')\n",
    "    \n",
    "    with open('auth_file.txt', 'r') as f: \n",
    "        client_id, secret_token, grant_type, username, password, user_agent = f.readline().split(\",\")\n",
    "        \n",
    "    # note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "    auth = requests.auth.HTTPBasicAuth(client_id, secret_token)\n",
    "\n",
    "    # here we pass our login method (password), username, and password\n",
    "    data = {'grant_type': grant_type,\n",
    "            'username': username,\n",
    "            'password': password}\n",
    "\n",
    "    # setup our header info, which gives reddit a brief description of our app\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    # send our request for an OAuth token\n",
    "    res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                        auth=auth, data=data, headers=headers)\n",
    "\n",
    "    # convert response to JSON and pull access_token value\n",
    "    TOKEN = res.json()['access_token']\n",
    "\n",
    "    # add authorization to our headers dictionary\n",
    "    headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "    \n",
    "    return headers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02092b98-7c22-4889-ac9f-0b413f733177",
   "metadata": {},
   "source": [
    "## POSTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc07d68-2722-4a5e-adf7-acd33f751097",
   "metadata": {},
   "source": [
    "### Extraction des posts depuis la requête"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becf2887-aba6-4af9-8540-3426e7f7fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_response_for_posts(res, date_extraction):\n",
    "    \"\"\"\n",
    "    We use this function to convert responses json to dataframes, in the context of POSTS extraction.\n",
    "    In this dataframe, we extract metrics from the json file given as a response\n",
    "    \n",
    "    Arg : res, which is the query we make to the Reddit API (a GET query).\n",
    "    Returns : a dataframe containing the info for each extracted post\n",
    "    \"\"\"\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # loop through each post pulled from res and append to df\n",
    "    for post in res.json()['data']['children']:\n",
    "        author = None\n",
    "        if 'author_fullname' in post['data']:\n",
    "            author = post['data']['author_fullname']\n",
    "        df_new = pd.DataFrame({\n",
    "            'id_post': post['data']['id'],\n",
    "            'id_subreddit': post['data']['subreddit_id'],\n",
    "            'subreddit': post['data']['subreddit'],\n",
    "            'id_author': author,\n",
    "            'author': post['data']['author'],\n",
    "            'num_comments': post['data']['num_comments'],\n",
    "            'subreddit_subscribers': post['data']['subreddit_subscribers'],\n",
    "            'upvote_ratio': post['data']['upvote_ratio'],\n",
    "            'ups': post['data']['ups'],\n",
    "            'downs': post['data']['downs'],\n",
    "            'score': post['data']['score'],\n",
    "            'created_utc': datetime.fromtimestamp(post['data']['created_utc']).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'extraction_utc': date_extraction,\n",
    "            'kind': post['kind'],\n",
    "            'score_jigsaw': None,\n",
    "        }, index=[1])\n",
    "        df = pd.concat([df, df_new], ignore_index=True)\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ba55ca-216b-4fbe-87e0-812f75cffb68",
   "metadata": {},
   "source": [
    "### Extraction complète des posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf3fc95-193c-4a63-afba-726587f702b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def posts_extraction(subreddit:str):\n",
    "    \"\"\"\n",
    "    Function to extract posts from a subreddit page.\n",
    "    \n",
    "    Args :\n",
    "    - subreddit: name of the subreddit under format \"name\" \n",
    "    \"\"\"\n",
    "\n",
    "    # We first authenticate to the API\n",
    "    headers = authenticate_API()\n",
    "\n",
    "    # Initialize dataframe and parameters for pulling data in loop \n",
    "    data = pd.DataFrame()\n",
    "    params = {'limit': 100}\n",
    "\n",
    "    # Create a flag for scanning the subreddit as long as there is a post to fetch\n",
    "    flag = True\n",
    "\n",
    "    # At each loop, we extract 100 posts with their info\n",
    "    while flag:\n",
    "        # make request\n",
    "        res = requests.get(f\"https://oauth.reddit.com/r/{subreddit}\",\n",
    "                        headers=headers,\n",
    "                        params=params)\n",
    "\n",
    "        # get dataframe from response\n",
    "        new_df = df_from_response_for_posts(res, date_extraction)\n",
    "        # take the final row (oldest entry)\n",
    "        row = new_df.iloc[len(new_df)-1]\n",
    "        # create fullname\n",
    "        fullname = row['kind'] + '_' + row['id_post']\n",
    "        # add/update fullname in params\n",
    "        params['after'] = fullname\n",
    "\n",
    "        # append new_df to data\n",
    "        data = pd.concat([data, new_df], ignore_index=True)\n",
    "\n",
    "        # Flag set to True if len(new_df)>=100, False otherwise\n",
    "        flag = (len(new_df)>=100)\n",
    "    \n",
    "    # Data insertion\n",
    "    insertion_bigquery('dwh', 'post', data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40c04d1-13b5-49a1-8c10-64c8d1be7a8d",
   "metadata": {},
   "source": [
    "## COMMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2043615-3d85-4336-8fa4-b122c31a76be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_info_commentaires(resjson, id_post, author):\n",
    "    \"\"\"\n",
    "    Permet d'extraire les informations des commentaires contenus dans le json\n",
    "    Return : le dataframe contenant les informations des commentaires\n",
    "    \"\"\"\n",
    "    # on ajoute le contenu du json au dataframe\n",
    "    df_add = pd.DataFrame({\n",
    "            'id_comment': resjson['data']['id'],\n",
    "            'id_post': id_post,\n",
    "            'id_author': author,\n",
    "            'author': resjson['data']['author'],\n",
    "            'content': resjson['data']['body'].replace(\"[supprimé]\",\"\").replace(\"[effacé]\",\"\"),\n",
    "            'type_content': 'Comment',\n",
    "            'ups': resjson['data']['ups'],\n",
    "            'downs': resjson['data']['downs'],\n",
    "            'score': resjson['data']['score'],\n",
    "            'created_utc': datetime.fromtimestamp(resjson['data']['created_utc']).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'extraction_utc': date_extraction,\n",
    "            'score_jigsaw': None,\n",
    "        }, index=[1])\n",
    "    \n",
    "    return df_add\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98feb9f-c749-4a48-b0ff-1074e14c3f8f",
   "metadata": {},
   "source": [
    "### Extraction des réponses aux commentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01611f65-6626-4765-b7db-c094f7464010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_replies(resjson, df_comments, id_post, headers):\n",
    "    \"\"\"\n",
    "    Permet d'extraire les réponses aux commentaires contenus dans le json resjson\n",
    "\n",
    "    Params :\n",
    "        - resjson : json\n",
    "        - df_comments : dataframe\n",
    "        - id_post : str\n",
    "        - headers :\n",
    "    \n",
    "    Return : le dataframe contenant les informations des réponses aux commentaires\n",
    "    \"\"\"\n",
    "    \n",
    "    global listeChildren\n",
    "    \n",
    "     # on ajoute le contenu du json au dataframe\n",
    "    author = None\n",
    "    if 'author_fullname' in resjson['data']:\n",
    "        author = resjson['data']['author_fullname']\n",
    "    if \"body\" in resjson['data']:\n",
    "        df_add = df_info_commentaires(resjson, id_post, author)\n",
    "        df_comments = pd.concat([df_comments, df_add], ignore_index=True)\n",
    "    else:\n",
    "        # on complète la listeChildren avec les id des commentaires non accessibles avec la première requête\n",
    "        listeChildren += resjson['data']['children']\n",
    "\n",
    "    # On vérifie s'il y a ou non une réponse aux commentaires\n",
    "    if \"replies\" in resjson['data']:\n",
    "        # recursive to add replies\n",
    "        if resjson['data']['replies'] != \"\":\n",
    "            for post in resjson['data']['replies']['data']['children']:\n",
    "                df_comments = df_replies(post, df_comments, id_post, headers)\n",
    "    else:\n",
    "        listeChildren += resjson['data']['children']\n",
    "\n",
    "    return df_comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665c213e-b4cb-438c-9cf6-978e38275b29",
   "metadata": {},
   "source": [
    "### Extraction des commentaires depuis la requête"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5965cff-b99b-461c-8588-673e450ae1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_info_titre_post(res, id_post, author, typeContent):\n",
    "    \"\"\"\n",
    "    Renvoie un dataframe contenant les informations du titre ou du contenu du post selon le type\n",
    "    \"\"\"\n",
    "    if typeContent == \"title\":\n",
    "        df_add = pd.DataFrame({\n",
    "            'id_comment': res.json()[0]['data']['children'][0]['data']['id']+\"t\",\n",
    "            'id_post': id_post,\n",
    "            'id_author': author,\n",
    "            'author': res.json()[0]['data']['children'][0]['data']['author'],\n",
    "            'content': res.json()[0]['data']['children'][0]['data']['title'].replace(\"[supprimé]\",\"\").replace(\"[effacé]\",\"\"),\n",
    "            'type_content': 'Title',\n",
    "            'ups': res.json()[0]['data']['children'][0]['data']['ups'],\n",
    "            'downs': res.json()[0]['data']['children'][0]['data']['downs'],\n",
    "            'score': res.json()[0]['data']['children'][0]['data']['score'],\n",
    "            'created_utc': datetime.fromtimestamp(res.json()[0]['data']['children'][0]['data']['created_utc']).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'extraction_utc': date_extraction,\n",
    "            'score_jigsaw': None,\n",
    "        }, index = [1])\n",
    "    else:\n",
    "        df_add = pd.DataFrame({\n",
    "            'id_comment': res.json()[0]['data']['children'][0]['data']['id']+\"p\",\n",
    "            'id_post': id_post,\n",
    "            'id_author': author,\n",
    "            'author': res.json()[0]['data']['children'][0]['data']['author'],\n",
    "            'content': res.json()[0]['data']['children'][0]['data']['selftext'].replace(\"[supprimé]\",\"\").replace(\"[effacé]\",\"\"),\n",
    "            'type_content': 'Post',\n",
    "            'ups': res.json()[0]['data']['children'][0]['data']['ups'],\n",
    "            'downs': res.json()[0]['data']['children'][0]['data']['downs'],\n",
    "            'score': res.json()[0]['data']['children'][0]['data']['score'],\n",
    "            'created_utc': datetime.fromtimestamp(res.json()[0]['data']['children'][0]['data']['created_utc']).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'extraction_utc': date_extraction,\n",
    "            'score_jigsaw': None,\n",
    "        }, index = [1])\n",
    "\n",
    "    return df_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1519aa7-1d4a-4a10-ba44-0bb0b2dd6a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_response_for_comments(res, headers, id_post):\n",
    "    \"\"\"\n",
    "    Permet d'extraire les commentaires contenus dans le json res\n",
    "\n",
    "    Params :\n",
    "        - res : json\n",
    "\n",
    "    Return : le dataframe contenant les informations des commentaires\n",
    "    \"\"\"\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df_comments = pd.DataFrame()\n",
    "    \n",
    "    # add_type title of the post\n",
    "    author = None\n",
    "    if 'author_fullname' in res.json()[0]['data']['children'][0]['data']:\n",
    "        author = res.json()[0]['data']['children'][0]['data']['author_fullname']\n",
    "    \n",
    "    df_add = df_info_titre_post(res, id_post, author, \"title\")\n",
    "    df_comments = pd.concat([df_comments, df_add], ignore_index=True)\n",
    "\n",
    "    # add_type content of the post\n",
    "    author = None\n",
    "    if 'author_fullname' in res.json()[0]['data']['children'][0]['data']:\n",
    "        author = res.json()[0]['data']['children'][0]['data']['author_fullname']\n",
    "    \n",
    "    df_add = df_info_titre_post(res, id_post, author, \"post\")\n",
    "    df_comments = pd.concat([df_comments, df_add], ignore_index=True)\n",
    "\n",
    "    # loop to add content of the comments\n",
    "    for post in res.json()[1]['data']['children']:\n",
    "        df_comments = df_replies(post, df_comments, id_post, headers)\n",
    "        \n",
    "    return df_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5cc60c-9ed8-44a2-98b7-7f581e8facbd",
   "metadata": {},
   "source": [
    "### Extraction des enfants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee2af5a-6f4a-42fd-8fde-c9e0b82b3143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_response_for_children(res, id_post, rang, headers):\n",
    "    \"\"\"\n",
    "    Permet d'extraire les commentaires contenus dans le json res\n",
    "\n",
    "    Params :\n",
    "        - res : json\n",
    "        - id_post : str\n",
    "        - rang : int\n",
    "\n",
    "    Return : le dataframe contenant les informations des commentaires\n",
    "    \"\"\"\n",
    "    \n",
    "    global listeChildren\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df_children = pd.DataFrame()\n",
    "    \n",
    "    if 'json' in res.json():\n",
    "        if len(res.json()['json']['data']['things']) != 0:\n",
    "            for post in res.json()['json']['data']['things']:\n",
    "                author = None\n",
    "                if 'author_fullname' in post['data']:\n",
    "                    author = post['data']['author_fullname']\n",
    "                if 'body' in post['data']:\n",
    "                    df_add = df_info_commentaires(post, id_post, author)\n",
    "                    df_children = pd.concat([df_children, df_add], ignore_index=True)\n",
    "\n",
    "    return df_children\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39340db7-cb1d-42f5-ba0e-425fd0b4d5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def children_extraction(df_comments, link_id, rang, headers, id_post):\n",
    "    \"\"\"\n",
    "    Permet d'extraire les commentaires non accessibles avec la première requête\n",
    "    Utilisation de l'API avec la requêtre 'more children'\n",
    "\n",
    "    Return : le dataframe avec les nouveaux commentaires\n",
    "    \"\"\"\n",
    "    children = \"\"\n",
    "    i = 0\n",
    "    global listeChildren\n",
    "\n",
    "    while len(listeChildren) > 0:\n",
    "        if len(listeChildren) > 100:\n",
    "            children = listeChildren[0]\n",
    "            for i in range(1, 100):\n",
    "                children += \", \"\n",
    "                children += listeChildren[i]\n",
    "            del(listeChildren[0:100])\n",
    "        else:\n",
    "            children = listeChildren[0]\n",
    "            for i in range(1, len(listeChildren)):\n",
    "                children += \", \"\n",
    "                children += listeChildren[i]\n",
    "            listeChildren.clear()\n",
    "\n",
    "        params = {'link_id': \"t3_\" + id_post, 'children': children, 'api_type': \"json\"}\n",
    "\n",
    "        # make request\n",
    "        res = requests.get(\"http://oauth.reddit.com/api/morechildren\",\n",
    "                            headers=headers, params=params)\n",
    "\n",
    "        # get dataframe from response\n",
    "        new_df = df_from_response_for_children(res, id_post, rang, headers)\n",
    "\n",
    "        # append new_df to data\n",
    "        df_comments = pd.concat([df_comments, new_df], ignore_index=True)\n",
    "\n",
    "    return df_comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef260f-dffe-4bd0-8362-2fb362adcffd",
   "metadata": {},
   "source": [
    "### Extraction complète des commentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e286b6-9b67-40a1-aecb-00d8229d1e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comments_extraction(subreddit:str, id_post:str):\n",
    "    \"\"\"\n",
    "    Permet d'extraire tous les commentaires d'un post et de les stocker dans un fichier csv\n",
    "\n",
    "    Params :\n",
    "        - subreddit : str\n",
    "        - id_post : str\n",
    "    \"\"\"\n",
    "    # We first authenticate to the API\n",
    "    headers = authenticate_API()\n",
    "\n",
    "    # initialize dataframe and parameters for pulling data in loop\n",
    "    data = pd.DataFrame()\n",
    "    params = {'limit': None}\n",
    "    \n",
    "    url = f\"https://oauth.reddit.com/{subreddit}/comments/{id_post}\"\n",
    "\n",
    "    # make request\n",
    "    res = requests.get(url,\n",
    "                    headers=headers,\n",
    "                    params=params)\n",
    "    \n",
    "    # get dataframe from response\n",
    "    new_df = df_from_response_for_comments(res, headers, id_post)\n",
    "\n",
    "    # append new_df to data\n",
    "    data = pd.concat([data, new_df], ignore_index=True)\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df = children_extraction(new_df, \"t3_\" + id_post, 1, headers, id_post)\n",
    "    \n",
    "    data = pd.concat([data, new_df], ignore_index=True)\n",
    "\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
