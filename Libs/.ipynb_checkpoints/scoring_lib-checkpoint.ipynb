{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7b694a5-6c7f-496a-9950-68173e3dcaa2",
   "metadata": {},
   "source": [
    "# Librairie pour scorer les contenus de Reddit\n",
    "\n",
    "* __Description__: Librairie pour scorer les contenus de Reddit\n",
    "* __Auteur__: Corentin TIMAL et Amin ALI ABDILLAHI\n",
    "* __Source__: Table comment (BigQuery)\n",
    "* __Output__: Table comment (BigQuery) \n",
    "* __Date de création__: 15/09/2022\n",
    "* __Date de mise à jour__: 16/09/2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835900d0-3f18-416f-9225-346c616ca31c",
   "metadata": {},
   "source": [
    "## Import des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e2a7ead9-85e2-43a9-978f-058904cc9ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from google.cloud import bigquery\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import pickle\n",
    "# import pandas_gbq\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79429a00-33ae-4f4e-9afb-ef799495bc6d",
   "metadata": {},
   "source": [
    "## Création du client pour la connexion BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dc8f637-650a-4d9f-b21e-ef848f36b9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb9cb44-bb3a-4d18-a43f-3d1be3fdc1a8",
   "metadata": {},
   "source": [
    "## Téléchargement des listes de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65a1d3c0-7194-4ef8-be54-a7c59779cd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "# Récupération des données pour le nettoyage\n",
    "stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "all_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913d31fa-237c-4039-a4a2-21f6bf9e3a70",
   "metadata": {},
   "source": [
    "## Nettoyage des contenus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bfe9199-a246-4fa9-bde3-f4c5ba22df91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(comment):\n",
    "    \"\"\"\n",
    "    Fonction pour préparer le commentaire à l'analyse\n",
    "    On enlève la ponctuation, les majuscules, les mots non pertinents\n",
    "    \"\"\"\n",
    "    comment = re.sub('[^a-zA-Z]', ' ', comment)\n",
    "    comment = comment.lower()\n",
    "    comment = comment.split()\n",
    "    comment = [stemmer.stem(word) for word in comment if not word in set(all_stopwords)]\n",
    "    comment = [lemmatizer.lemmatize(word) for word in comment]\n",
    "    comment = ' '.join(comment)\n",
    "    return comment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79de795-972e-442f-9843-1db518bd15c1",
   "metadata": {},
   "source": [
    "## Existence du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7440d046-eb90-4cc9-a5d2-31173bfb1dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_exist(path):\n",
    "    \"\"\"\n",
    "    Fonction qui vérifie si le chemin d'accès existe\n",
    "    \"\"\"\n",
    "    return os.path.exists(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3cb281-7162-4ba8-94a1-7816d673b9f9",
   "metadata": {},
   "source": [
    "## Entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0373fca2-0265-411e-8aa4-6f1223078a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_type_model(type_model, path_train_data):\n",
    "    # Création du dataframe contenant les données d'entraînement notées selon le type d'insultes\n",
    "    df = pd.read_csv(path_train_data)\n",
    "    # Choix de la colonne à analyser\n",
    "    if type_model == 'toxic':\n",
    "        df['y'] = df['toxic'].astype(int)\n",
    "    elif type_model == 'severe_toxic':\n",
    "        df['y'] = df['severe_toxic'].astype(int)\n",
    "    elif type_model == 'obscene':\n",
    "        df['y'] = df['obscene'].astype(int)\n",
    "    elif type_model == 'threat':\n",
    "        df['y'] = df['threat'].astype(int)\n",
    "    elif type_model == 'insult':\n",
    "        df['y'] = df['insult'].astype(int)\n",
    "    elif type_model == 'identity_hate':\n",
    "        df['y'] = df['identity_hate'].astype(int)\n",
    "    else:\n",
    "        df['y'] = (df[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1)>0).astype(int)\n",
    "            \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea2dbe26-6f5d-4527-8829-936a48fe3226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_train_model(df):\n",
    "    \"\"\"\n",
    "    Fonction pour créer le dataframe d'entraînement\n",
    "    \"\"\"\n",
    "    # Suppression des colonnes inutiles\n",
    "    df = df[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\n",
    "    # Normalisation des insultes\n",
    "    df['y'].value_counts(normalize=True)\n",
    "    # Nombre de lignes avec un score à 1\n",
    "    min_len = (df['y'] == 1).sum()\n",
    "    # Création de l'échantillon avec un score à 0\n",
    "    df_y0_undersample = df[df['y'] == 0].sample(n=min_len, random_state=201)\n",
    "    # Création du dataframe d'entraînement\n",
    "    df = pd.concat([df[df['y'] == 1], df_y0_undersample])\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8caa4e61-7747-45de-9a01-d50085667f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df, path_validation_data, path_vec):\n",
    "    \"\"\"\n",
    "    Fonction qui entraine le modele à partir du dataframe\n",
    "    \"\"\"\n",
    "    print(\"Training model\")\n",
    "    vec = TfidfVectorizer()\n",
    "    # Nettoyage du dataframe\n",
    "    df['text'] = df['text'].apply(clean)\n",
    "    # Vectorisation du dataframe\n",
    "    X = vec.fit_transform(df['text'])\n",
    "    # Création du modèle\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X, df['y'])\n",
    "    \n",
    "    # Création du dataframe de validation du modèle\n",
    "    df_val = pd.read_csv(path_validation_data)\n",
    "    # Vectorisation du dataframe\n",
    "    X_less_toxic = vec.transform(df_val['less_toxic'].apply(clean))\n",
    "    X_more_toxic = vec.transform(df_val['more_toxic'].apply(clean))\n",
    "    # Prédiction\n",
    "    p1 = model.predict_proba(X_less_toxic)\n",
    "    p2 = model.predict_proba(X_more_toxic)\n",
    "    # Validation\n",
    "    mean = (p1[:, 1] < p2[:, 1]).mean()\n",
    "    \n",
    "    print(\"Sauvegarde du vec\", path_vec)\n",
    "    save_model(vec, path_vec)\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8b7ee722-e8eb-4e34-80fe-6ae2351db64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_name):\n",
    "    \"\"\"\n",
    "    Fonction qui sauvegarde le modele \n",
    "    \"\"\"\n",
    "    print(\"Save model at:\", file_name)\n",
    "    with open(file_name,\"wb\") as file:\n",
    "        pickle.dump(model, file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9ae4a9-aa6a-4beb-be6f-2f4bd5ebf360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(file_model):\n",
    "    \"\"\"\n",
    "    Fonction qui recupere un modele sauvegardé\n",
    "    \"\"\"\n",
    "    with open(file_model,\"rb\") as file:\n",
    "        model = pickle.load(file)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1e15ba36-17e5-4b31-bc3f-75c019d610fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_ML(type_model, path_train_data, path_validation_data, file_name_model, path_vec):\n",
    "    \"\"\"\n",
    "    Fonction qui entraine, sauvegarde et valide le modele de machine learning\n",
    "    \"\"\"\n",
    "    print(\"Training...\", type_model)\n",
    "    # Choix du type de modèle\n",
    "    df = df_type_model(type_model, path_train_data)\n",
    "    # Création du dataframe d'entraînement\n",
    "    df = df_train_model(df)\n",
    "    # Entrainement du modèle\n",
    "    model = train_model(df, path_validation_data, path_vec)\n",
    "    # Sauvegarde du modèle\n",
    "    save_model(model, file_name_model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3858a0a-bd04-4946-b0cc-c4af5522e9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(df_param, model, vec, table):\n",
    "    \"\"\"\n",
    "    Fonction qui applique le modele sur nos données\n",
    "    \"\"\"\n",
    "    df = df_param.copy()\n",
    "    # Nettoyage des données\n",
    "    df['content'] = df['content'].apply(clean)\n",
    "    # Vectorisation\n",
    "    X_test = vec.transform(df['content'])\n",
    "    # Application du modèle\n",
    "    p3 = model.predict_proba(X_test)\n",
    "    # Enregistrement des résultats dans le dataframe\n",
    "    df['score_jigsaw'] = p3[:,1]\n",
    "    del df['content']\n",
    "    df['score_jigsaw'] = df['score_jigsaw'].apply(lambda x:1-x)\n",
    "    return df\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
