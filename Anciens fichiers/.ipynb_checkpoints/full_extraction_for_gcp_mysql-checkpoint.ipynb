{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5b0ea73",
   "metadata": {},
   "source": [
    "# Notebook pour réaliser l'extraction des posts et commentaires\n",
    "### Adapté pour être facilement utilisable sur Jupyter dans GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05f49a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58f3904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "from google.cloud import storage\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "import sys\n",
    "import csv\n",
    "import emoji\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542f3153-d960-425a-a63e-57c365270b8f",
   "metadata": {},
   "source": [
    "## Importer un fichier depuis le bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a78a834e-6517-4339-ad82-150b84d7f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The ID of your GCS object\n",
    "    # source_blob_name = \"storage-object-name\"\n",
    "\n",
    "    # The path to which the file should be downloaded\n",
    "    # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Construct a client side representation of a blob.\n",
    "    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "    # any content from Google Cloud Storage. As we don't need additional data,\n",
    "    # using `Bucket.blob` is preferred here.\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ff523",
   "metadata": {},
   "source": [
    "## Fonction d'authentification  à l'API reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7228b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_API():\n",
    "    \"\"\"\n",
    "    Function to authenticate to the Reddit API\n",
    "    Returns the headers we'll need for future API queries.\n",
    "    \"\"\"\n",
    "    # Open the file containing the login infos, and store them for later on\n",
    "    download_blob('the-clean-project','notebooks/jupyter/authentication_file.txt','auth_file.txt')\n",
    "    \n",
    "    with open('auth_file.txt', 'r') as f: \n",
    "        client_id, secret_token, grant_type, username, password, user_agent = f.readline().split(\",\")\n",
    "        \n",
    "    # note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "    auth = requests.auth.HTTPBasicAuth(client_id, secret_token)\n",
    "\n",
    "    # here we pass our login method (password), username, and password\n",
    "    data = {'grant_type': grant_type,\n",
    "            'username': username,\n",
    "            'password': password}\n",
    "\n",
    "    # setup our header info, which gives reddit a brief description of our app\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    # send our request for an OAuth token\n",
    "    res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                        auth=auth, data=data, headers=headers)\n",
    "\n",
    "    # convert response to JSON and pull access_token value\n",
    "    TOKEN = res.json()['access_token']\n",
    "\n",
    "    # add authorization to our headers dictionary\n",
    "    headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "    return headers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52773f75",
   "metadata": {},
   "source": [
    "## Le code d'extraction des posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ea5d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_response_for_posts(res, date_extraction):\n",
    "    \"\"\"\n",
    "    We use this function to convert responses to dataframes, in the context of POSTS extraction.\n",
    "    In this dataframe, we extract metrics from the json file given as a response\n",
    "    \n",
    "    Arg : res, which is the query we make to the Reddit API (a GET query).\n",
    "    Returns : a dataframe containing the info for each extracted post, that is (for now):\n",
    "    - subreddit: subreddit name;\n",
    "    - title: post title;\n",
    "    - selftext: post body;\n",
    "    - author_fullname: id of the post author (t2_'xxxxx');\n",
    "    - upvote_ratio; \n",
    "    - created_utc: publication time;\n",
    "    - num_comments;\n",
    "    - id: post ID, which is part of each post URL btw;\n",
    "    - kind: type prefix, for posts of a subreddit it is \"t3\", \"t1\" for a comment...\n",
    "    \"\"\"\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # loop through each post pulled from res and append to df\n",
    "    for post in res.json()['data']['children']:\n",
    "        author = None\n",
    "        if 'author_fullname' in post['data']:\n",
    "            author = post['data']['author_fullname']\n",
    "        df_new = pd.DataFrame({\n",
    "            'id_post': post['data']['id'],\n",
    "            'id_subreddit': post['data']['subreddit_id'],\n",
    "            'subreddit': post['data']['subreddit'],\n",
    "            'id_author': author,\n",
    "            'author': post['data']['author'],\n",
    "            'num_comments': post['data']['num_comments'],\n",
    "            'subreddit_subscribers': post['data']['subreddit_subscribers'],\n",
    "            'upvote_ratio': post['data']['upvote_ratio'],\n",
    "            'ups': post['data']['ups'],\n",
    "            'downs': post['data']['downs'],\n",
    "            'score': post['data']['score'],\n",
    "            'created_utc': datetime.fromtimestamp(post['data']['created_utc']).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'extraction_utc': date_extraction,\n",
    "            'kind': post['kind']\n",
    "        }, index=[1])\n",
    "        df = pd.concat([df, df_new], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def posts_extraction(subreddit:str=\"r/worldnews\"):\n",
    "    \"\"\"\n",
    "    Function to extract the first posts from a subreddit page.\n",
    "    \n",
    "    Args :\n",
    "    - subreddit: name of the subreddit under format \"r/name\", by default \"r/worldnews\";\n",
    "    - n_posts: int, number of posts we want to extract from the subreddit, by default 100.\n",
    "        A choice we make here is to automatically round this number to the upper hundred in our code.\n",
    "    Creates : a csv file containing the info about the extracted posts, that is:\n",
    "    -\n",
    "    -\n",
    "    -\n",
    "\n",
    "    TODO: try-except, error prevention \n",
    "    \"\"\"\n",
    "\n",
    "    # We first authenticate to the API\n",
    "    headers = authenticate_API()\n",
    "\n",
    "    # initialize dataframe and parameters for pulling data in loop \n",
    "    data = pd.DataFrame()\n",
    "    params = {'limit': 10}\n",
    "\n",
    "    # Create a flag for scanning the subreddit as long as there is a post to fetch\n",
    "    flag = True\n",
    "#     date_string = str(datetime.now())\n",
    "    date_extraction = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # At each loop, we extract 100 posts with their info\n",
    "#     while flag:\n",
    "#         # make request\n",
    "#         res = requests.get(f\"https://oauth.reddit.com/r/{subreddit}\",\n",
    "#                         headers=headers,\n",
    "#                         params=params)\n",
    "\n",
    "#         # get dataframe from response\n",
    "#         new_df = df_from_response_for_posts(res, date_extraction)\n",
    "#         # take the final row (oldest entry)\n",
    "#         row = new_df.iloc[len(new_df)-1]\n",
    "#         # create fullname\n",
    "#         fullname = row['kind'] + '_' + row['id_post']\n",
    "#         # add/update fullname in params\n",
    "#         params['after'] = fullname\n",
    "        \n",
    "#         # append new_df to data\n",
    "#         # data = data.append(new_df, ignore_index=True)\n",
    "#         data = pd.concat([data, new_df], ignore_index=True)\n",
    "\n",
    "#         # Flag set to True if len(new_df)>=100, False otherwise\n",
    "#         flag = (len(new_df)>=100)\n",
    "    \n",
    "    # make request\n",
    "    res = requests.get(f\"https://oauth.reddit.com/r/{subreddit}\",\n",
    "                    headers=headers,\n",
    "                    params=params)\n",
    "\n",
    "    # get dataframe from response\n",
    "    new_df = df_from_response_for_posts(res, date_extraction)\n",
    "    # take the final row (oldest entry)\n",
    "    row = new_df.iloc[len(new_df)-1]\n",
    "    # create fullname\n",
    "    fullname = row['kind'] + '_' + row['id_post']\n",
    "    # add/update fullname in params\n",
    "    params['after'] = fullname\n",
    "\n",
    "    # append new_df to data\n",
    "    # data = data.append(new_df, ignore_index=True)\n",
    "    data = pd.concat([data, new_df], ignore_index=True)\n",
    "\n",
    "    # Flag set to True if len(new_df)>=100, False otherwise\n",
    "    flag = (len(new_df)>=100)\n",
    "    \n",
    "    # Connexion à la base de données\n",
    "    #cnx = mysql.connector.connect(user='root', password='tcp',host='34.155.60.153',database='dwh')\n",
    "    # Connexion base v3\n",
    "    cnx = mysql.connector.connect(user='root', password='tcp',host='34.77.244.3',database='dwh')\n",
    "    cursor=cnx.cursor()\n",
    "\n",
    "    # creating column list for insertion\n",
    "    cols = \"`,`\".join([str(i) for i in data.columns.tolist()])\n",
    "\n",
    "    # Insert DataFrame recrds one by one.\n",
    "    for i, row in data.iterrows():\n",
    "        sql = \"INSERT INTO `post` (`\" +cols + \"`) VALUES (\" + \"%s,\"*(len(row)-1) + \"%s)\"\n",
    "        cursor.execute(sql, tuple(row))\n",
    "\n",
    "        # the connection is not autocommitted by default, so we must commit to save our changes\n",
    "        cnx.commit()\n",
    "    \n",
    "    return date_extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c12dde8",
   "metadata": {},
   "source": [
    "## Le code d'extraction des commentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bfd5f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_replies(resjson, df_comments, id_post, rang, headers):\n",
    "    \"\"\"\n",
    "    Permet d'extraire les réponses aux commentaires contenus dans le json resjson\n",
    "\n",
    "    Requête : comments\n",
    "\n",
    "    Params :\n",
    "        - resjson : json\n",
    "        - df_comments : dataframe\n",
    "        - id_post : str\n",
    "        - rang : int\n",
    "    \n",
    "    Return : le dataframe contenant les informations des réponses aux commentaires\n",
    "    \"\"\"\n",
    "    # add content of the comment in the dataframe\n",
    "    author = None\n",
    "    if 'author_fullname' in resjson['data']:\n",
    "        author = resjson['data']['author_fullname']\n",
    "    if \"body\" in resjson['data']:\n",
    "        df_add = pd.DataFrame({\n",
    "            'id_comment': resjson['data']['id'],\n",
    "            'id_post': id_post,\n",
    "            'id_author': author,\n",
    "            'author': resjson['data']['author'],\n",
    "            'content': emoji.demojize(resjson['data']['body']),\n",
    "            'type_content': 'Comment',\n",
    "            'ups': resjson['data']['ups'],\n",
    "            'downs': resjson['data']['downs'],\n",
    "            'score': resjson['data']['score'],\n",
    "            'created_utc': datetime.fromtimestamp(resjson['data']['created_utc']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        }, index=[1])\n",
    "\n",
    "        df_comments = pd.concat([df_comments, df_add], ignore_index=True)\n",
    "    else:\n",
    "        df_comments = children_extraction(df_comments, resjson['data']['id'],resjson['data']['children'], rang + 1, headers, id_post)\n",
    "\n",
    "    if \"replies\" in resjson['data']:\n",
    "        # recursive to add replies\n",
    "        if resjson['data']['replies'] != \"\":\n",
    "            for post in resjson['data']['replies']['data']['children']:\n",
    "                df_comments = df_replies(post, df_comments, id_post, rang + 1, headers)\n",
    "    else:\n",
    "        df_comments = children_extraction(df_comments, resjson['data']['id'],resjson['data']['children'], rang + 1, headers, id_post)\n",
    "\n",
    "    return df_comments\n",
    "\n",
    "def df_from_response_for_comments(res, headers, id_post):\n",
    "    \"\"\"\n",
    "    Permet d'extraire les commentaires contenus dans le json res\n",
    "\n",
    "    Requête : comments\n",
    "\n",
    "    Params :\n",
    "        - res : json\n",
    "\n",
    "    Return : le dataframe contenant les informations des commentaires\n",
    "    \"\"\"\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df_comments = pd.DataFrame()\n",
    "    \n",
    "    # add_type title of the post\n",
    "    author = None\n",
    "    if 'author_fullname' in res.json()[0]['data']['children'][0]['data']:\n",
    "        author = res.json()[0]['data']['children'][0]['data']['author_fullname']\n",
    "    df_add = pd.DataFrame({\n",
    "        'id_comment': res.json()[0]['data']['children'][0]['data']['id'],\n",
    "        'id_post': id_post,\n",
    "        'id_author': author,\n",
    "        'author': res.json()[0]['data']['children'][0]['data']['author'],\n",
    "        'content': emoji.demojize(res.json()[0]['data']['children'][0]['data']['title']),\n",
    "        'type_content': 'Title',\n",
    "        'ups': res.json()[0]['data']['children'][0]['data']['ups'],\n",
    "        'downs': res.json()[0]['data']['children'][0]['data']['downs'],\n",
    "        'score': res.json()[0]['data']['children'][0]['data']['score'],\n",
    "        'created_utc': datetime.fromtimestamp(res.json()[0]['data']['children'][0]['data']['created_utc']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }, index = [1])\n",
    "\n",
    "    df_comments = pd.concat([df_comments, df_add], ignore_index=True)\n",
    "\n",
    "    # add_type content of the post\n",
    "    author = None\n",
    "    if 'author_fullname' in res.json()[0]['data']['children'][0]['data']:\n",
    "        author = res.json()[0]['data']['children'][0]['data']['author_fullname']\n",
    "    df_add = pd.DataFrame({\n",
    "        'id_comment': res.json()[0]['data']['children'][0]['data']['id'],\n",
    "        'id_post': id_post,\n",
    "        'id_author': author,\n",
    "        'author': res.json()[0]['data']['children'][0]['data']['author'],\n",
    "        'content': emoji.demojize(res.json()[0]['data']['children'][0]['data']['selftext']),\n",
    "        'type_content': 'Post',\n",
    "        'ups': res.json()[0]['data']['children'][0]['data']['ups'],\n",
    "        'downs': res.json()[0]['data']['children'][0]['data']['downs'],\n",
    "        'score': res.json()[0]['data']['children'][0]['data']['score'],\n",
    "        'created_utc': datetime.fromtimestamp(res.json()[0]['data']['children'][0]['data']['created_utc']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }, index = [1])\n",
    "\n",
    "    df_comments = pd.concat([df_comments, df_add], ignore_index=True)\n",
    "\n",
    "    # loop to add content of the comments\n",
    "    for post in res.json()[1]['data']['children']:\n",
    "        df_comments = df_replies(post, df_comments, id_post, 1, headers)\n",
    "    return df_comments\n",
    "\n",
    "def df_from_response_for_children(res, id_post, rang, headers):\n",
    "    \"\"\"\n",
    "    Permet d'extraire les commentaires contenus dans le json res\n",
    "\n",
    "    Requête : morechildren\n",
    "\n",
    "    Params :\n",
    "        - res : json\n",
    "        - id_post : str\n",
    "        - rang : int\n",
    "\n",
    "    Return : le dataframe contenant les informations des commentaires\n",
    "    \"\"\"\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df_children = pd.DataFrame()\n",
    "    \n",
    "    if 'json' in res.json():\n",
    "        if len(res.json()['json']['data']['things']) != 0:\n",
    "            for post in res.json()['json']['data']['things']:\n",
    "                author = None\n",
    "                if 'author_fullname' in post['data']:\n",
    "                    author = post['data']['author_fullname']\n",
    "                if 'body' in post['data']:\n",
    "                    df_add = pd.DataFrame({\n",
    "                        'id_comment': post['data']['id'],\n",
    "                        'id_post': id_post,\n",
    "                        'id_author': author,\n",
    "                        'author': post['data']['author'],\n",
    "                        'content': emoji.demojize(post['data']['body']),\n",
    "                        'type_content': 'Comment',\n",
    "                        'ups': post['data']['ups'],\n",
    "                        'downs': post['data']['downs'],\n",
    "                        'score': post['data']['score'],\n",
    "                        'created_utc': datetime.fromtimestamp(post['data']['created_utc']).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    }, index=[1])\n",
    "                    df_children = pd.concat([df_children, df_add], ignore_index=True)\n",
    "                else:\n",
    "                    df_children = children_extraction(df_children, post['data']['id'], post['data']['children'], rang + 1, headers, id_post)\n",
    "\n",
    "    return df_children\n",
    "\n",
    "def children_extraction(df_comments, link_id, liste_children, rang, headers, id_post):\n",
    "    \"\"\"\n",
    "    Permet d'extraire les commentaires non accessibles avec la première requête\n",
    "    Utilisation de l'API avec la requêtre 'more children'\n",
    "\n",
    "    Params :\n",
    "        - df_comments : dataframe\n",
    "        - link_id : str\n",
    "        - children : list\n",
    "        - rang : int\n",
    "\n",
    "    Return : le dataframe avec les nouveaux commentaires\n",
    "    \"\"\"\n",
    "    children = \"\"\n",
    "    i = 0\n",
    "\n",
    "    while len(liste_children) > 0:\n",
    "        if len(liste_children) > 100:\n",
    "            children = liste_children[0]\n",
    "            for i in range(1, 100):\n",
    "                children += \", \"\n",
    "                children += liste_children[i]\n",
    "            del(liste_children[0:100])\n",
    "        else:\n",
    "            children = liste_children[0]\n",
    "            for i in range(1, len(liste_children)):\n",
    "                children += \", \"\n",
    "                children += liste_children[i]\n",
    "            liste_children.clear()\n",
    "\n",
    "        params = {'link_id': \"t3_\" + id_post, 'children': children, 'api_type': \"json\"}\n",
    "\n",
    "        # make request\n",
    "        res = requests.get(\"http://oauth.reddit.com/api/morechildren\",\n",
    "                            headers=headers, params=params)\n",
    "\n",
    "        # get dataframe from response\n",
    "        new_df = df_from_response_for_children(res, id_post, rang, headers)\n",
    "\n",
    "        # append new_df to data\n",
    "        df_comments = pd.concat([df_comments, new_df], ignore_index=True)\n",
    "\n",
    "    return df_comments\n",
    "\n",
    "def comments_extraction(subreddit:str, id_post:str, date_extraction):\n",
    "    \"\"\"\n",
    "    Permet d'extraire tous les commentaires d'un post et de les stocker dans un fichier csv\n",
    "\n",
    "    Params :\n",
    "        - subreddit : str\n",
    "        - id_post : str\n",
    "    \"\"\"\n",
    "    # We first authenticate to the API\n",
    "    headers = authenticate_API()\n",
    "\n",
    "    # initialize dataframe and parameters for pulling data in loop\n",
    "    data = pd.DataFrame()\n",
    "    params = {'limit': None}\n",
    "\n",
    "    # make request\n",
    "    res = requests.get(f\"https://oauth.reddit.com/{subreddit}/comments/{id_post}\",\n",
    "                    headers=headers,\n",
    "                    params=params)\n",
    "    \n",
    "    # get dataframe from response\n",
    "    new_df = df_from_response_for_comments(res, headers, id_post)\n",
    "\n",
    "    # append new_df to data\n",
    "    data = pd.concat([data, new_df], ignore_index=True)\n",
    "#     data['content'] = data['content'].astype(\"string\")\n",
    "\n",
    "    # Connexion à la base de données\n",
    "    #cnx = mysql.connector.connect(user='root', password='tcp',host='34.155.60.153',database='dwh')\n",
    "    # Connexion base v3\n",
    "    cnx = mysql.connector.connect(user='root', password='tcp',host='34.77.244.3',database='dwh')\n",
    "    cursor=cnx.cursor()\n",
    "\n",
    "    # creating column list for insertion\n",
    "    cols = \"`,`\".join([str(i) for i in data.columns.tolist()])\n",
    "\n",
    "#     Insert DataFrame records one by one.\n",
    "    for i, row in data.iterrows():\n",
    "        try:\n",
    "            sql = \"INSERT INTO `comment` (`\" +cols + \"`,`extraction_utc`) VALUES (\" + \"%s,\"*(len(row)) + \"%s);\"\n",
    "#         print(type(sql), sql)\n",
    "           # print(type(tuple(row)), tuple(row))\n",
    "#             print(sql)\n",
    "            listeValues = list(tuple(row))\n",
    "            listeValues.append(date_extraction)\n",
    "            cursor.execute(sql, listeValues)\n",
    "        # the connection is not autocommitted by default, so we must commit to save our changes\n",
    "            cnx.commit()\n",
    "        except:\n",
    "            print(type(tuple(row)[4]), tuple(row)[4])\n",
    "            pass\n",
    "    \n",
    "#     # Insert DataFrame recrds one by one.\n",
    "#     for i, row in data.iterrows():\n",
    "#         sql = \"INSERT INTO `comment` (`\" +cols + \"`,`extraction_utc`) VALUES (\" + \"%s,\"*(len(row)) + \"%s);\"\n",
    "# #         print(type(sql), sql)\n",
    "#         print(type(tuple(row)[4]), tuple(row)[4])\n",
    "# #         print(sql)\n",
    "#         listeValues = list(tuple(row))\n",
    "#         listeValues.append(date_extraction)\n",
    "#         cursor.execute(sql, listeValues)\n",
    "# #     # the connection is not autocommitted by default, so we must commit to save our changes\n",
    "#     cnx.commit()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b91717",
   "metadata": {},
   "source": [
    "## L'extraction complète"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b244710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraction_complete(subreddit:str=\"worldnews\"):\n",
    "    \"\"\"\n",
    "        Fonction qui, quand on lui fournit le subreddit, va récupérer de façon automatique :\n",
    "        - ses posts;\n",
    "        - les commentaires associés à chacun des posts;\n",
    "        (- la liste de ses contributeurs, ainsi que\n",
    "        - leur activité récente respective) (abandonné : demande trop de requêtes API pour l'instant...)\n",
    "\n",
    "        param:\n",
    "            subreddit: str, nom du subreddit au format \"r/nom_subreddit\"\n",
    "    \n",
    "        Output:\n",
    "            Enregistre les données ainsi extraites dans des fichiers csv.\n",
    "\n",
    "        \n",
    "        #TODO :\n",
    "            - on récupère aussi les json ?\n",
    "            - prévoir d'organiser le système de fichiers où les sauvegarder (bucket dans GCP?)\n",
    "    \"\"\"\n",
    "    # Connexion à la base de données\n",
    "#     cnx = mysql.connector.connect(user='root', password='tcp',host='34.155.60.153',database='dwh')\n",
    "    # Connexion base v3\n",
    "    cnx = mysql.connector.connect(user='root', password='tcp',host='34.77.244.3',database='dwh')\n",
    "    cursor = cnx.cursor()\n",
    "    \n",
    "#     # Extraire les posts du subreddit\n",
    "    date_extraction = posts_extraction(subreddit)\n",
    "    \n",
    "#     # creating column list for insertion\n",
    "#     cols = \"`,`\".join([str(i) for i in data.columns.tolist()])\n",
    "\n",
    "#     # Insert DataFrame recrds one by one.\n",
    "#     for i, row in data.iterrows():\n",
    "#         sql = \"INSERT INTO `post` (`\" +cols + \"`) VALUES (\" + \"%s,\"*(len(row)-1) + \"%s)\"\n",
    "#         cursor.execute(sql, tuple(row))\n",
    "\n",
    "#         # the connection is not autocommitted by default, so we must commit to save our changes\n",
    "#         cnx.commit()\n",
    "    sql = f\"SELECT id_post FROM post WHERE subreddit = '{subreddit}' AND extraction_utc = '{date_extraction}';\"\n",
    "    cursor.execute(sql)\n",
    "    #récupèrer toutes les lignes de la dernière instruction exécutée.\n",
    "    res = cursor.fetchall()\n",
    "    print(date_extraction)\n",
    "#     compteur = 0\n",
    "    for line in res:\n",
    "#         print(line)\n",
    "#         print(\"r/\"+subreddit, line[0], date_extraction)\n",
    "#         compteur += comments_extraction(\"r/\"+subreddit, line[0], date_extraction, compteur)\n",
    "        comments_extraction(\"r/\"+subreddit, line[0], date_extraction)\n",
    "    \n",
    "#     print(compteur)\n",
    "    \n",
    "#     df = pd.read_csv(posts_filename, storage_options={\"token\": \"cloud\"})\n",
    "    \n",
    "#     for i in df.index:\n",
    "# #         print(i)\n",
    "#         comments_extraction(subreddit, df[\"id\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d688ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extraction_complete(\"r/fcbasel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec57d348-c396-4618-b632-75e3d59074cd",
   "metadata": {},
   "source": [
    "## Import des données des 30 plus gros subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6be38cf2-6db5-4acf-99c3-e855cd4e6e65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top30Subreddits = [\"r/announcements\", \"r/funny\", \"r/AskReddit\", \"r/gaming\", \"r/aww\",\n",
    "                \"r/Music\", \"r/pics\", \"r/science\", \"r/worldnews\", \"r/videos\",\n",
    "                \"r/todayilearned\", \"r/movies\", \"r/news\", \"r/Showerthoughts\", \"r/EarthPorn\",\n",
    "                \"r/gifs\", \"r/IAmA\", \"r/food\", \"r/askscience\", \"r/Jokes\",\n",
    "                \"r/LifeProTips\", \"r/explainlikeimfive\", \"r/Art\", \"r/books\", \"r/mildlyinteresting\",\n",
    "                \"r/nottheonion\", \"r/DIY\", \"r/sports\", \"r/blog\", \"r/space\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3d2d67c-92f2-4049-af1e-98da9b5ba9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-09-09 08:30:57.720672'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d8c1b8-8a7e-4ab4-ab3d-4aa228d0b2d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-09 08:30:58\n"
     ]
    }
   ],
   "source": [
    "extraction_complete(\"worldnews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb7e5e7-5424-4aec-9e7e-f6b5c8f5dcb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-09-09 08:55:59'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be3bfd2-b578-414f-bd48-50e90ff524db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11350842-1b81-49cd-979b-7b0cbb8d3865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a913a-7ebf-4640-a557-9907df1a8041",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
