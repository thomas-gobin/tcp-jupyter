{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5b0ea73",
   "metadata": {},
   "source": [
    "# Notebook pour réaliser l'extraction des posts et commentaires\n",
    "### Adapté pour être facilement utilisable sur Jupyter dans GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58f3904f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import sqlite3\n",
    "from google.cloud import storage\n",
    "import numpy as np\n",
    "import mysql.connector\n",
    "import sys\n",
    "import csv\n",
    "import time\n",
    "from google.cloud import bigquery\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a98f44c1-73cb-479a-b8fa-f6b2b62fa2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542f3153-d960-425a-a63e-57c365270b8f",
   "metadata": {},
   "source": [
    "## Importer un fichier depuis le bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a78a834e-6517-4339-ad82-150b84d7f94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    # The ID of your GCS bucket\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "\n",
    "    # The ID of your GCS object\n",
    "    # source_blob_name = \"storage-object-name\"\n",
    "\n",
    "    # The path to which the file should be downloaded\n",
    "    # destination_file_name = \"local/path/to/file\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    # Construct a client side representation of a blob.\n",
    "    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
    "    # any content from Google Cloud Storage. As we don't need additional data,\n",
    "    # using `Bucket.blob` is preferred here.\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "    blob.download_to_filename(destination_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ff523",
   "metadata": {},
   "source": [
    "## Fonction d'authentification  à l'API reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7228b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_API():\n",
    "    \"\"\"\n",
    "    Function to authenticate to the Reddit API\n",
    "    Returns the headers we'll need for future API queries.\n",
    "    \"\"\"\n",
    "    # Open the file containing the login infos, and store them for later on\n",
    "    download_blob('the-clean-project','notebooks/jupyter/authentication_file.txt','auth_file.txt')\n",
    "    \n",
    "    with open('auth_file.txt', 'r') as f: \n",
    "        client_id, secret_token, grant_type, username, password, user_agent = f.readline().split(\",\")\n",
    "        \n",
    "    # note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "    auth = requests.auth.HTTPBasicAuth(client_id, secret_token)\n",
    "\n",
    "    # here we pass our login method (password), username, and password\n",
    "    data = {'grant_type': grant_type,\n",
    "            'username': username,\n",
    "            'password': password}\n",
    "\n",
    "    # setup our header info, which gives reddit a brief description of our app\n",
    "    headers = {'User-Agent': user_agent}\n",
    "\n",
    "    # send our request for an OAuth token\n",
    "    res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                        auth=auth, data=data, headers=headers)\n",
    "\n",
    "    # convert response to JSON and pull access_token value\n",
    "    TOKEN = res.json()['access_token']\n",
    "\n",
    "    # add authorization to our headers dictionary\n",
    "    headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "    return headers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52773f75",
   "metadata": {},
   "source": [
    "## Le code d'extraction des posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ea5d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_response_for_posts(res, date_extraction):\n",
    "    \"\"\"\n",
    "    We use this function to convert responses to dataframes, in the context of POSTS extraction.\n",
    "    In this dataframe, we extract metrics from the json file given as a response\n",
    "    \n",
    "    Arg : res, which is the query we make to the Reddit API (a GET query).\n",
    "    Returns : a dataframe containing the info for each extracted post, that is (for now):\n",
    "    - subreddit: subreddit name;\n",
    "    - title: post title;\n",
    "    - selftext: post body;\n",
    "    - author_fullname: id of the post author (t2_'xxxxx');\n",
    "    - upvote_ratio; \n",
    "    - created_utc: publication time;\n",
    "    - num_comments;\n",
    "    - id: post ID, which is part of each post URL btw;\n",
    "    - kind: type prefix, for posts of a subreddit it is \"t3\", \"t1\" for a comment...\n",
    "    \"\"\"\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    # loop through each post pulled from res and append to df\n",
    "    for post in res.json()['data']['children']:\n",
    "        author = None\n",
    "        if 'author_fullname' in post['data']:\n",
    "            author = post['data']['author_fullname']\n",
    "        df_new = pd.DataFrame({\n",
    "            'id_post': post['data']['id'],\n",
    "            'id_subreddit': post['data']['subreddit_id'],\n",
    "            'subreddit': post['data']['subreddit'],\n",
    "            'id_author': author,\n",
    "            'author': post['data']['author'],\n",
    "            'num_comments': post['data']['num_comments'],\n",
    "            'subreddit_subscribers': post['data']['subreddit_subscribers'],\n",
    "            'upvote_ratio': post['data']['upvote_ratio'],\n",
    "            'ups': post['data']['ups'],\n",
    "            'downs': post['data']['downs'],\n",
    "            'score': post['data']['score'],\n",
    "            'created_utc': datetime.fromtimestamp(post['data']['created_utc']).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'extraction_utc': date_extraction,\n",
    "            'kind': post['kind'],\n",
    "        }, index=[1])\n",
    "\n",
    "        df = pd.concat([df, df_new], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def posts_extraction(subreddit:str=\"r/worldnews\"):\n",
    "    \"\"\"\n",
    "    Function to extract the first posts from a subreddit page.\n",
    "    \n",
    "    Args :\n",
    "    - subreddit: name of the subreddit under format \"r/name\", by default \"r/worldnews\";\n",
    "    - n_posts: int, number of posts we want to extract from the subreddit, by default 100.\n",
    "        A choice we make here is to automatically round this number to the upper hundred in our code.\n",
    "    Creates : a csv file containing the info about the extracted posts, that is:\n",
    "    -\n",
    "    -\n",
    "    -\n",
    "\n",
    "    TODO: try-except, error prevention \n",
    "    \"\"\"\n",
    "\n",
    "    # We first authenticate to the API\n",
    "    headers = authenticate_API()\n",
    "\n",
    "    # initialize dataframe and parameters for pulling data in loop \n",
    "    data = pd.DataFrame()\n",
    "    params = {'limit': 100}\n",
    "\n",
    "    # Create a flag for scanning the subreddit as long as there is a post to fetch\n",
    "    flag = True\n",
    "\n",
    "    # At each loop, we extract 100 posts with their info\n",
    "    while flag:\n",
    "        # make request\n",
    "        res = requests.get(f\"https://oauth.reddit.com/r/{subreddit}\",\n",
    "                        headers=headers,\n",
    "                        params=params)\n",
    "\n",
    "        # get dataframe from response\n",
    "        new_df = df_from_response_for_posts(res, date_extraction)\n",
    "        # take the final row (oldest entry)\n",
    "        row = new_df.iloc[len(new_df)-1]\n",
    "        # create fullname\n",
    "        fullname = row['kind'] + '_' + row['id_post']\n",
    "        # add/update fullname in params\n",
    "        params['after'] = fullname\n",
    "\n",
    "        # append new_df to data\n",
    "        data = pd.concat([data, new_df], ignore_index=True)\n",
    "\n",
    "        # Flag set to True if len(new_df)>=100, False otherwise\n",
    "        flag = (len(new_df)>=100)\n",
    "    \n",
    "    dataset_id = 'dwh'\n",
    "    # For this sample, the table must already exist and have a defined schema\n",
    "    table_id = 'post'\n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    table = client.get_table(table_ref)\n",
    "    # Creating a list of tuples with the values that shall be inserted into the table\n",
    "    client.insert_rows(table, data.values.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c12dde8",
   "metadata": {},
   "source": [
    "## Le code d'extraction des commentaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bfd5f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "listeChildren = []\n",
    "\n",
    "def df_replies(resjson, df_comments, id_post, rang, headers):\n",
    "    \"\"\"\n",
    "    Permet d'extraire les réponses aux commentaires contenus dans le json resjson\n",
    "\n",
    "    Requête : comments\n",
    "\n",
    "    Params :\n",
    "        - resjson : json\n",
    "        - df_comments : dataframe\n",
    "        - id_post : str\n",
    "        - rang : int\n",
    "    \n",
    "    Return : le dataframe contenant les informations des réponses aux commentaires\n",
    "    \"\"\"\n",
    "    \n",
    "    global listeChildren\n",
    "    # add content of the comment in the dataframe\n",
    "    author = None\n",
    "    if 'author_fullname' in resjson['data']:\n",
    "        author = resjson['data']['author_fullname']\n",
    "    if \"body\" in resjson['data']:\n",
    "        df_add = pd.DataFrame({\n",
    "            'id_comment': resjson['data']['id'],\n",
    "            'id_post': id_post,\n",
    "            'id_author': author,\n",
    "            'author': resjson['data']['author'],\n",
    "            'content': resjson['data']['body'],\n",
    "            'type_content': 'Comment',\n",
    "            'ups': resjson['data']['ups'],\n",
    "            'downs': resjson['data']['downs'],\n",
    "            'score': resjson['data']['score'],\n",
    "            'created_utc': datetime.fromtimestamp(resjson['data']['created_utc']).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'extraction_utc': date_extraction,\n",
    "        }, index=[1])\n",
    "\n",
    "        df_comments = pd.concat([df_comments, df_add], ignore_index=True)\n",
    "    else:\n",
    "        listeChildren += resjson['data']['children']\n",
    "#         df_comments = children_extraction(df_comments, resjson['data']['id'],resjson['data']['children'], rang + 1, headers, id_post)\n",
    "\n",
    "    if \"replies\" in resjson['data']:\n",
    "        # recursive to add replies\n",
    "        if resjson['data']['replies'] != \"\":\n",
    "            for post in resjson['data']['replies']['data']['children']:\n",
    "                df_comments = df_replies(post, df_comments, id_post, rang + 1, headers)\n",
    "    else:\n",
    "        listeChildren += resjson['data']['children']\n",
    "#         df_comments = children_extraction(df_comments, resjson['data']['id'],resjson['data']['children'], rang + 1, headers, id_post)\n",
    "\n",
    "    return df_comments\n",
    "\n",
    "def df_from_response_for_comments(res, headers, id_post):\n",
    "    \"\"\"\n",
    "    Permet d'extraire les commentaires contenus dans le json res\n",
    "\n",
    "    Requête : comments\n",
    "\n",
    "    Params :\n",
    "        - res : json\n",
    "\n",
    "    Return : le dataframe contenant les informations des commentaires\n",
    "    \"\"\"\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df_comments = pd.DataFrame()\n",
    "    \n",
    "    # add_type title of the post\n",
    "    author = None\n",
    "    if 'author_fullname' in res.json()[0]['data']['children'][0]['data']:\n",
    "        author = res.json()[0]['data']['children'][0]['data']['author_fullname']\n",
    "    df_add = pd.DataFrame({\n",
    "        'id_comment': res.json()[0]['data']['children'][0]['data']['id'],\n",
    "        'id_post': id_post,\n",
    "        'id_author': author,\n",
    "        'author': res.json()[0]['data']['children'][0]['data']['author'],\n",
    "        'content': res.json()[0]['data']['children'][0]['data']['title'],\n",
    "        'type_content': 'Title',\n",
    "        'ups': res.json()[0]['data']['children'][0]['data']['ups'],\n",
    "        'downs': res.json()[0]['data']['children'][0]['data']['downs'],\n",
    "        'score': res.json()[0]['data']['children'][0]['data']['score'],\n",
    "        'created_utc': datetime.fromtimestamp(res.json()[0]['data']['children'][0]['data']['created_utc']).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'extraction_utc': date_extraction,\n",
    "    }, index = [1])\n",
    "\n",
    "    df_comments = pd.concat([df_comments, df_add], ignore_index=True)\n",
    "\n",
    "    # add_type content of the post\n",
    "    author = None\n",
    "    if 'author_fullname' in res.json()[0]['data']['children'][0]['data']:\n",
    "        author = res.json()[0]['data']['children'][0]['data']['author_fullname']\n",
    "    df_add = pd.DataFrame({\n",
    "        'id_comment': res.json()[0]['data']['children'][0]['data']['id'],\n",
    "        'id_post': id_post,\n",
    "        'id_author': author,\n",
    "        'author': res.json()[0]['data']['children'][0]['data']['author'],\n",
    "        'content': res.json()[0]['data']['children'][0]['data']['selftext'],\n",
    "        'type_content': 'Post',\n",
    "        'ups': res.json()[0]['data']['children'][0]['data']['ups'],\n",
    "        'downs': res.json()[0]['data']['children'][0]['data']['downs'],\n",
    "        'score': res.json()[0]['data']['children'][0]['data']['score'],\n",
    "        'created_utc': datetime.fromtimestamp(res.json()[0]['data']['children'][0]['data']['created_utc']).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'extraction_utc': date_extraction,\n",
    "    }, index = [1])\n",
    "\n",
    "    df_comments = pd.concat([df_comments, df_add], ignore_index=True)\n",
    "\n",
    "    # loop to add content of the comments\n",
    "    for post in res.json()[1]['data']['children']:\n",
    "        df_comments = df_replies(post, df_comments, id_post, 1, headers)\n",
    "    return df_comments\n",
    "\n",
    "def df_from_response_for_children(res, id_post, rang, headers):\n",
    "    \"\"\"\n",
    "    Permet d'extraire les commentaires contenus dans le json res\n",
    "\n",
    "    Requête : morechildren\n",
    "\n",
    "    Params :\n",
    "        - res : json\n",
    "        - id_post : str\n",
    "        - rang : int\n",
    "\n",
    "    Return : le dataframe contenant les informations des commentaires\n",
    "    \"\"\"\n",
    "    \n",
    "    global listeChildren\n",
    "    # initialize temp dataframe for batch of data in response\n",
    "    df_children = pd.DataFrame()\n",
    "    \n",
    "    if 'json' in res.json():\n",
    "        if len(res.json()['json']['data']['things']) != 0:\n",
    "            for post in res.json()['json']['data']['things']:\n",
    "                author = None\n",
    "                if 'author_fullname' in post['data']:\n",
    "                    author = post['data']['author_fullname']\n",
    "                if 'body' in post['data']:\n",
    "                    df_add = pd.DataFrame({\n",
    "                        'id_comment': post['data']['id'],\n",
    "                        'id_post': id_post,\n",
    "                        'id_author': author,\n",
    "                        'author': post['data']['author'],\n",
    "                        'content': post['data']['body'],\n",
    "                        'type_content': 'Comment',\n",
    "                        'ups': post['data']['ups'],\n",
    "                        'downs': post['data']['downs'],\n",
    "                        'score': post['data']['score'],\n",
    "                        'created_utc': datetime.fromtimestamp(post['data']['created_utc']).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                        'extraction_utc': date_extraction,\n",
    "                    }, index=[1])\n",
    "\n",
    "                    df_children = pd.concat([df_children, df_add], ignore_index=True)\n",
    "#                 else:\n",
    "#                     listeChildren += post['data']['children']\n",
    "#                     df_children = children_extraction(df_children, post['data']['id'], rang + 1, headers, id_post)\n",
    "\n",
    "    return df_children\n",
    "\n",
    "def children_extraction(df_comments, link_id, rang, headers, id_post):\n",
    "    \"\"\"\n",
    "    Permet d'extraire les commentaires non accessibles avec la première requête\n",
    "    Utilisation de l'API avec la requêtre 'more children'\n",
    "\n",
    "    Params :\n",
    "        - df_comments : dataframe\n",
    "        - link_id : str\n",
    "        - children : list\n",
    "        - rang : int\n",
    "\n",
    "    Return : le dataframe avec les nouveaux commentaires\n",
    "    \"\"\"\n",
    "    children = \"\"\n",
    "    i = 0\n",
    "    global listeChildren\n",
    "\n",
    "    while len(listeChildren) > 0:\n",
    "        if len(listeChildren) > 100:\n",
    "            children = listeChildren[0]\n",
    "            for i in range(1, 100):\n",
    "                children += \", \"\n",
    "                children += listeChildren[i]\n",
    "            del(listeChildren[0:100])\n",
    "        else:\n",
    "            children = listeChildren[0]\n",
    "            for i in range(1, len(listeChildren)):\n",
    "                children += \", \"\n",
    "                children += listeChildren[i]\n",
    "            listeChildren.clear()\n",
    "\n",
    "        params = {'link_id': \"t3_\" + id_post, 'children': children, 'api_type': \"json\"}\n",
    "\n",
    "        # make request\n",
    "        res = requests.get(\"http://oauth.reddit.com/api/morechildren\",\n",
    "                            headers=headers, params=params)\n",
    "\n",
    "        # get dataframe from response\n",
    "        new_df = df_from_response_for_children(res, id_post, rang, headers)\n",
    "\n",
    "        # append new_df to data\n",
    "        df_comments = pd.concat([df_comments, new_df], ignore_index=True)\n",
    "\n",
    "    return df_comments\n",
    "\n",
    "def comments_extraction(subreddit:str, id_post:str):\n",
    "    \"\"\"\n",
    "    Permet d'extraire tous les commentaires d'un post et de les stocker dans un fichier csv\n",
    "\n",
    "    Params :\n",
    "        - subreddit : str\n",
    "        - id_post : str\n",
    "    \"\"\"\n",
    "    # We first authenticate to the API\n",
    "    headers = authenticate_API()\n",
    "\n",
    "    # initialize dataframe and parameters for pulling data in loop\n",
    "    data = pd.DataFrame()\n",
    "    params = {'limit': None}\n",
    "    \n",
    "    url = f\"https://oauth.reddit.com/{subreddit}/comments/{id_post}\"\n",
    "\n",
    "    # make request\n",
    "    res = requests.get(url,\n",
    "                    headers=headers,\n",
    "                    params=params)\n",
    "    \n",
    "#     new_df = pd.DataFrame({\n",
    "#         'json': res\n",
    "#     })\n",
    "\n",
    "#     get dataframe from response\n",
    "    new_df = df_from_response_for_comments(res, headers, id_post)\n",
    "\n",
    "#     append new_df to data\n",
    "    data = pd.concat([data, new_df], ignore_index=True)\n",
    "    data['content'] = data['content'].astype(\"string\")\n",
    "\n",
    "    new_df = pd.DataFrame()\n",
    "    new_df = children_extraction(new_df, \"t3_\" + id_post, 1, headers, id_post)\n",
    "    \n",
    "    data = pd.concat([data, new_df], ignore_index=True)\n",
    "    \n",
    "#     dataset_id = 'dwh'\n",
    "#     # For this sample, the table must already exist and have a defined schema\n",
    "#     table_id = 'comment'\n",
    "#     table_ref = client.dataset(dataset_id).table(table_id)\n",
    "#     table = client.get_table(table_ref)\n",
    "#     # Creating a list of tuples with the values that shall be inserted into the table\n",
    "#     client.insert_rows(table, data.values.tolist())\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b91717",
   "metadata": {},
   "source": [
    "## L'extraction complète"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3b244710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraction_complete(subreddit:str=\"worldnews\"):\n",
    "    \"\"\"\n",
    "        Fonction qui, quand on lui fournit le subreddit, va récupérer de façon automatique :\n",
    "        - ses posts;\n",
    "        - les commentaires associés à chacun des posts;\n",
    "        (- la liste de ses contributeurs, ainsi que\n",
    "        - leur activité récente respective) (abandonné : demande trop de requêtes API pour l'instant...)\n",
    "\n",
    "        param:\n",
    "            subreddit: str, nom du subreddit au format \"r/nom_subreddit\"\n",
    "    \n",
    "        Output:\n",
    "            Enregistre les données ainsi extraites dans des fichiers csv.\n",
    "\n",
    "        \n",
    "        #TODO :\n",
    "            - on récupère aussi les json ?\n",
    "            - prévoir d'organiser le système de fichiers où les sauvegarder (bucket dans GCP?)\n",
    "    \"\"\" \n",
    "    \n",
    "    posts_extraction(subreddit)\n",
    "    \n",
    "    # Recup BigQuery\n",
    "    date = date_extraction[:10]\n",
    "    heure = date_extraction[11:]\n",
    "    \n",
    "    sql = f\"SELECT id_post FROM `mimetic-coral-355913.dwh.post` WHERE extraction_utc = '{date}T{heure}'\"\n",
    "    \n",
    "    # BigQuerry to Pandas DataFrame\n",
    "    pandas_df = client.query(sql).to_dataframe()\n",
    "    # Pandas DF to Spark DataFrame\n",
    "    spark_df = spark.createDataFrame(pandas_df)\n",
    "    # Spark DF to Spark RDD\n",
    "    rdd = spark_df.rdd\n",
    "    # flatmap pour faire qlqch\n",
    "    rdd2 = rdd.map(lambda row : comments_extraction(f\"r/{subreddit}\", row[\"id_post\"]))\n",
    "    rdd3 = rdd2.reduce(lambda df1, df2: pd.concat([df1, df2]))\n",
    "    \n",
    "    dataset_id = 'dwh'\n",
    "    # For this sample, the table must already exist and have a defined schema\n",
    "    table_id = 'comment'\n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    table = client.get_table(table_ref)\n",
    "    # Creating a list of tuples with the values that shall be inserted into the table\n",
    "    client.insert_rows(table, rdd3.values.tolist())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4937807d-92b3-40e6-a1a0-0dcbaa0ef05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = date_extraction[:10]\n",
    "heure = date_extraction[11:]\n",
    "\n",
    "sql = f\"SELECT id_post FROM `mimetic-coral-355913.dwh.post` WHERE extraction_utc = '2022-09-12T14:58:11'\"\n",
    "\n",
    "# BigQuerry to Pandas DataFrame\n",
    "pandas_df = client.query(sql).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "948248e4-34a4-4bc6-8db7-a1a48da545ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "subreddit = \"PlutoTV\"\n",
    "# Pandas DF to Spark DataFrame\n",
    "spark_df = spark.createDataFrame(results)\n",
    "# Spark DF to Spark RDD\n",
    "rdd = spark_df.rdd\n",
    "# flatmap pour faire qlqch\n",
    "rdd2 = rdd.map(lambda row : comments_extraction(f\"r/{subreddit}\", row[\"id_post\"]))\n",
    "rdd3 = rdd2.reduce(lambda df1, df2: pd.concat([df1, df2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "91b0ab48-e245-4fce-9a3b-988ff7e8985d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_comment</th>\n",
       "      <th>id_post</th>\n",
       "      <th>id_author</th>\n",
       "      <th>author</th>\n",
       "      <th>content</th>\n",
       "      <th>type_content</th>\n",
       "      <th>ups</th>\n",
       "      <th>downs</th>\n",
       "      <th>score</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>extraction_utc</th>\n",
       "      <th>score_jigsaw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qlombx</td>\n",
       "      <td>qlombx</td>\n",
       "      <td>t2_a2fwzt60</td>\n",
       "      <td>roscoethehorse</td>\n",
       "      <td>Explicit lyrics</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-11-03 06:38:41</td>\n",
       "      <td>2022-09-12T14:58:11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qlombx</td>\n",
       "      <td>qlombx</td>\n",
       "      <td>t2_a2fwzt60</td>\n",
       "      <td>roscoethehorse</td>\n",
       "      <td>Is there a way to block songs with explicit ly...</td>\n",
       "      <td>Post</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-11-03 06:38:41</td>\n",
       "      <td>2022-09-12T14:58:11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>s5hymi</td>\n",
       "      <td>s5hymi</td>\n",
       "      <td>t2_7f8bjbav</td>\n",
       "      <td>Itchy-Throat-4779</td>\n",
       "      <td>BOycotting PlutoTV</td>\n",
       "      <td>Title</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-16 18:24:32</td>\n",
       "      <td>2022-09-12T14:58:11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s5hymi</td>\n",
       "      <td>s5hymi</td>\n",
       "      <td>t2_7f8bjbav</td>\n",
       "      <td>Itchy-Throat-4779</td>\n",
       "      <td>I am boycotting PLUTOTV for one month for infi...</td>\n",
       "      <td>Post</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-16 18:24:32</td>\n",
       "      <td>2022-09-12T14:58:11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hsy7ovs</td>\n",
       "      <td>s5hymi</td>\n",
       "      <td>t2_9jqle015</td>\n",
       "      <td>TDSinv</td>\n",
       "      <td>Lol what? They don’t make the ads 😂</td>\n",
       "      <td>Comment</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-01-16 21:09:46</td>\n",
       "      <td>2022-09-12T14:58:11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hjs8kup</td>\n",
       "      <td>qommxu</td>\n",
       "      <td>t2_14191a</td>\n",
       "      <td>KyloRenKardashian</td>\n",
       "      <td>Lol this is true. so much nostalgia</td>\n",
       "      <td>Comment</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2021-11-08 08:57:21</td>\n",
       "      <td>2022-09-12T14:58:11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hlqvhgi</td>\n",
       "      <td>qommxu</td>\n",
       "      <td>None</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>True that</td>\n",
       "      <td>Comment</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2021-11-23 08:15:21</td>\n",
       "      <td>2022-09-12T14:58:11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hkennx3</td>\n",
       "      <td>qommxu</td>\n",
       "      <td>t2_aw4mblxc</td>\n",
       "      <td>HaydenMilk</td>\n",
       "      <td>I'm just mad about all the glitches. The conte...</td>\n",
       "      <td>Comment</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-11-13 01:40:19</td>\n",
       "      <td>2022-09-12T14:58:11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hlvqfnv</td>\n",
       "      <td>qommxu</td>\n",
       "      <td>t2_13fip9</td>\n",
       "      <td>spunjbaf</td>\n",
       "      <td>I really agree. The FAVORITES function makes f...</td>\n",
       "      <td>Comment</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-11-24 09:05:05</td>\n",
       "      <td>2022-09-12T14:58:11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hpmql8w</td>\n",
       "      <td>qommxu</td>\n",
       "      <td>t2_hegisjh</td>\n",
       "      <td>Crazypandathe20th</td>\n",
       "      <td>I definitely agree. I love their live TV servi...</td>\n",
       "      <td>Comment</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2021-12-23 01:30:36</td>\n",
       "      <td>2022-09-12T14:58:11</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>594 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_comment id_post    id_author             author  \\\n",
       "0      qlombx  qlombx  t2_a2fwzt60     roscoethehorse   \n",
       "1      qlombx  qlombx  t2_a2fwzt60     roscoethehorse   \n",
       "0      s5hymi  s5hymi  t2_7f8bjbav  Itchy-Throat-4779   \n",
       "1      s5hymi  s5hymi  t2_7f8bjbav  Itchy-Throat-4779   \n",
       "2     hsy7ovs  s5hymi  t2_9jqle015             TDSinv   \n",
       "..        ...     ...          ...                ...   \n",
       "3     hjs8kup  qommxu    t2_14191a  KyloRenKardashian   \n",
       "4     hlqvhgi  qommxu         None          [deleted]   \n",
       "5     hkennx3  qommxu  t2_aw4mblxc         HaydenMilk   \n",
       "6     hlvqfnv  qommxu    t2_13fip9           spunjbaf   \n",
       "7     hpmql8w  qommxu   t2_hegisjh  Crazypandathe20th   \n",
       "\n",
       "                                              content type_content  ups  \\\n",
       "0                                     Explicit lyrics        Title    0   \n",
       "1   Is there a way to block songs with explicit ly...         Post    0   \n",
       "0                                  BOycotting PlutoTV        Title    0   \n",
       "1   I am boycotting PLUTOTV for one month for infi...         Post    0   \n",
       "2                 Lol what? They don’t make the ads 😂      Comment    5   \n",
       "..                                                ...          ...  ...   \n",
       "3                 Lol this is true. so much nostalgia      Comment    4   \n",
       "4                                           True that      Comment    3   \n",
       "5   I'm just mad about all the glitches. The conte...      Comment    2   \n",
       "6   I really agree. The FAVORITES function makes f...      Comment    2   \n",
       "7   I definitely agree. I love their live TV servi...      Comment    1   \n",
       "\n",
       "    downs  score          created_utc       extraction_utc score_jigsaw  \n",
       "0       0      0  2021-11-03 06:38:41  2022-09-12T14:58:11         None  \n",
       "1       0      0  2021-11-03 06:38:41  2022-09-12T14:58:11         None  \n",
       "0       0      0  2022-01-16 18:24:32  2022-09-12T14:58:11         None  \n",
       "1       0      0  2022-01-16 18:24:32  2022-09-12T14:58:11         None  \n",
       "2       0      5  2022-01-16 21:09:46  2022-09-12T14:58:11         None  \n",
       "..    ...    ...                  ...                  ...          ...  \n",
       "3       0      4  2021-11-08 08:57:21  2022-09-12T14:58:11         None  \n",
       "4       0      3  2021-11-23 08:15:21  2022-09-12T14:58:11         None  \n",
       "5       0      2  2021-11-13 01:40:19  2022-09-12T14:58:11         None  \n",
       "6       0      2  2021-11-24 09:05:05  2022-09-12T14:58:11         None  \n",
       "7       0      1  2021-12-23 01:30:36  2022-09-12T14:58:11         None  \n",
       "\n",
       "[594 rows x 12 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec57d348-c396-4618-b632-75e3d59074cd",
   "metadata": {},
   "source": [
    "## Import des données des 30 plus gros subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6be38cf2-6db5-4acf-99c3-e855cd4e6e65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top30Subreddits = [\"r/announcements\", \"r/funny\", \"r/AskReddit\", \"r/gaming\", \"r/aww\",\n",
    "                \"r/Music\", \"r/pics\", \"r/science\", \"r/worldnews\", \"r/videos\",\n",
    "                \"r/todayilearned\", \"r/movies\", \"r/news\", \"r/Showerthoughts\", \"r/EarthPorn\",\n",
    "                \"r/gifs\", \"r/IAmA\", \"r/food\", \"r/askscience\", \"r/Jokes\",\n",
    "                \"r/LifeProTips\", \"r/explainlikeimfive\", \"r/Art\", \"r/books\", \"r/mildlyinteresting\",\n",
    "                \"r/nottheonion\", \"r/DIY\", \"r/sports\", \"r/blog\", \"r/space\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a3d2d67c-92f2-4049-af1e-98da9b5ba9c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-09-14 09:12:28.403485'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "16d8c1b8-8a7e-4ab4-ab3d-4aa228d0b2d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-14 09:14:17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "date_extraction = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(date_extraction)\n",
    "extraction_complete(\"PlutoTV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0cb7e5e7-5424-4aec-9e7e-f6b5c8f5dcb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-09-14 09:15:20'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bee95f4-d5a0-492c-9959-2438a4510ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}